{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO_v3_ObjectDetection_TensorFlow_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1d_ttY-LcvytmmipO7iRzAYwGFHXQvCMj",
      "authorship_tag": "ABX9TyPAK2bvr5/RNVirmG5SUQ/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/CV-Yolo/blob/main/YOLO_v3_ObjectDetection_TensorFlow_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PIUHRF9dD8JW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import PIL\n",
        "import struct # used to convert native Python data types such as strings and numbers into a string of bytes and vice versa\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.merge import add, concatenate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirments"
      ],
      "metadata": {
        "id": "QOQcmZ3Jb28f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d aruchomu/data-for-yolo-v3-kernel\n",
        "!unzip \\*.zip && rm *.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8e42EPzMnXD",
        "outputId": "1801630a-b25a-4923-83ba-426717d25d28"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data-for-yolo-v3-kernel.zip to /content\n",
            " 90% 241M/267M [00:02<00:00, 114MB/s]\n",
            "100% 267M/267M [00:02<00:00, 127MB/s]\n",
            "Archive:  data-for-yolo-v3-kernel.zip\n",
            "  inflating: coco.names              \n",
            "  inflating: detections.gif          \n",
            "  inflating: dog.jpg                 \n",
            "  inflating: futur.ttf               \n",
            "  inflating: office.jpg              \n",
            "  inflating: yolov3.weights          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P model_data https://pjreddie.com/media/files/yolov3.weights\n",
        "\n",
        "!wget -P model_data https://pjreddie.com/media/files/yolov3-tiny.weights\n",
        "\n",
        "!wget -P model_data https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\n",
        "\n",
        "!wget -P model_data https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights"
      ],
      "metadata": {
        "id": "diEwCPUvb6Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Weights_Reader:\n",
        "\n",
        "\n",
        "  def __init__(self, filename):\n",
        "\n",
        "    with open(filename, 'rb') as w_file:\n",
        "      major, = struct.unpack('i', w_file.read(4))\n",
        "      minor, = struct.unpack('i', w_file.read(4))\n",
        "      revision, = struct.unpack('i', w_file.read(4))\n",
        "\n",
        "      if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
        "        w_file.read(8)\n",
        "      else:\n",
        "        w_file.read(4)\n",
        "      \n",
        "      transpose = (major>1000) or (minor>1000)\n",
        "      binary_weights = w_file.read()\n",
        "      \n",
        "    self.offset = 0\n",
        "    self.all_weights = np.frombuffer(binary_weights, dtype='float32')\n",
        "\n",
        "  def byte_reader(self, size):\n",
        "    self.offset +=  size\n",
        "    return self.all_weights[self.offset - size: self.offset]\n",
        "\n",
        "  \n",
        "  def load_weights(self, model):\n",
        "\n",
        "    for i in range(106): # 53*2 = 106 layers in total\n",
        "      try: # if it is a Convolutional Layer\n",
        "        conv_layer = model.get_layer(f'conv_{i}')\n",
        "        print(f'Loading weights of Convolution Layer {i}')\n",
        "\n",
        "        if i not in [81, 93, 105]: #if it is not a Detection Layer (82, 94, 106)\n",
        "         \n",
        "          norm_layer = model.get_layer(f'bnorm_{i}')\n",
        "          size = np.prod(norm_layer.get_weights()[0].shape) \n",
        "\n",
        "          bias = self.byte_reader(size)\n",
        "          scale = self.byte_reader(size)\n",
        "          mean = self.byte_reader(size)\n",
        "          variance = self.byte_reader(size)\n",
        "\n",
        "          weights = norm_layer.set_weights([scale, bias, mean, variance])\n",
        "\n",
        "        if len(conv_layer.get_weights()) > 1:\n",
        "          bias   = self.byte_reader(np.prod(conv_layer.get_weights()[1].shape))\n",
        "          kernel = self.byte_reader(np.prod(conv_layer.get_weights()[0].shape))\n",
        "          kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
        "          kernel = kernel.transpose([2, 3, 1, 0])\n",
        "          conv_layer.set_weights([kernel, bias])\n",
        "        else:\n",
        "          kernel = self.byte_reader(np.prod(conv_layer.get_weights()[0].shape))\n",
        "          kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
        "          kernel = kernel.transpose([2, 3, 1, 0])\n",
        "          conv_layer.set_weights([kernel])\n",
        "\n",
        "      except ValueError:\n",
        "        print(f\"Layer {i} is Not Convolution Layer\")\n",
        "\n",
        "        \n",
        "  def reset(self):\n",
        "    self.offset = 0   "
      ],
      "metadata": {
        "id": "FGzyHJKlSC3C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(inputs, convolutions, skip=True):\n",
        "\n",
        "    X = inputs\n",
        "    count = 0\n",
        "    total_convolutions = len(convolutions)\n",
        "    \n",
        "    for conv in convolutions:\n",
        "\n",
        "        if skip and count == (total_convolutions - 2):\n",
        "            skip_connection = X\n",
        "\n",
        "        count += 1\n",
        "        if conv['stride'] > 1 :  \n",
        "          X = tf.keras.layers.ZeroPadding2D(((1,0),(1,0)))(X) # peculiar padding as darknet prefers left and top\n",
        "\n",
        "\n",
        "        X = tf.keras.layers.Conv2D(filters = conv['filter'],\n",
        "                                    kernel_size = conv['kernel'],\n",
        "                                    strides = conv['stride'],\n",
        "                                    padding = 'valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefers left and top\n",
        "                                    use_bias = False if conv['bnorm'] else True,\n",
        "                                    name = f\"conv_{conv['layer_idx']}\")(X)\n",
        "\n",
        "\n",
        "        if conv['bnorm']:\n",
        "           X = tf.keras.layers.BatchNormalization(epsilon = 0.001, name = f\"bnorm_{conv['layer_idx']}\")(X)\n",
        "        if conv['leaky']:\n",
        "           X = tf.keras.layers.LeakyReLU(alpha = 0.1, name = f\"leaky_{conv['layer_idx']}\")(X)\n",
        "\n",
        "    return add([skip_connection, X]) if skip else X\n"
      ],
      "metadata": {
        "id": "ty0mOdbyulCG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_yolov3_model():\n",
        "\n",
        "    input_image = tf.keras.layers.Input(shape=(None, None, 3))\n",
        "\n",
        "    # Layers 0 to 4\n",
        "    x = conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n",
        "                                {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n",
        "                                {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n",
        "                                {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n",
        "\n",
        "    # Layers 5 to 8\n",
        "    x = conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n",
        "                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n",
        "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n",
        "\n",
        "    # Layers 9 to 11\n",
        "    x = conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n",
        "                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n",
        "\n",
        "    # Layers 12 to 15\n",
        "    x = conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n",
        "                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n",
        "                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n",
        "\n",
        "    # Layers 16 to 36\n",
        "    for i in range(7):\n",
        "        x = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n",
        "                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n",
        "    skip_36 = x\n",
        "    \n",
        "    # Layers 37 to 40\n",
        "    x = conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n",
        "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n",
        "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n",
        "\n",
        "    # Layers 41 to 61\n",
        "    for i in range(7):\n",
        "        x = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n",
        "                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n",
        "    skip_61 = x\n",
        "\n",
        "    # Layers 62 to 65\n",
        "    x = conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n",
        "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n",
        "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n",
        "\n",
        "    # Layers 66 to 74\n",
        "    for i in range(3):\n",
        "        x = conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n",
        "                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n",
        "\n",
        "    # Layers 75 to 79\n",
        "    x = conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n",
        "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n",
        "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n",
        "                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n",
        "                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n",
        "\n",
        "    # Layers 80 to 82\n",
        "    yolo_82 = conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n",
        "                             {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n",
        "\n",
        "    # Layers 83 to 86\n",
        "    x = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n",
        "    x = tf.keras.layers.UpSampling2D(2)(x)\n",
        "    x = concatenate([x, skip_61])\n",
        "\n",
        "    # Layers 87 to 91\n",
        "    x = conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n",
        "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n",
        "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n",
        "                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n",
        "                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n",
        "\n",
        "    # Layers 92 to 94\n",
        "    yolo_94 = conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n",
        "                                                                                     {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n",
        "\n",
        "    # Layers 95 to 98\n",
        "    x = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n",
        "    x = tf.keras.layers.UpSampling2D(2)(x)\n",
        "    x = concatenate([x, skip_36])\n",
        "\n",
        "    # Layers 99 to 106\n",
        "    yolo_106 = conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n",
        "                                {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n",
        "                                {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n",
        "                                {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n",
        "                                {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n",
        "                                {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n",
        "                                {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n",
        "\n",
        "    model = tf.keras.models.Model(input_image, [yolo_82, yolo_94, yolo_106])\n",
        "    return model"
      ],
      "metadata": {
        "id": "4IsgWrqfQGhl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Part"
      ],
      "metadata": {
        "id": "K6mO1eo1fzpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO options\n",
        "YOLO_TYPE                   = \"yolov3\" # yolov4 or yolov3\n",
        "YOLO_FRAMEWORK              = \"tf\" # \"tf\" or \"trt\"\n",
        "YOLO_V3_WEIGHTS             = \"model_data/yolov3.weights\"\n",
        "YOLO_V4_WEIGHTS             = \"model_data/yolov4.weights\"\n",
        "YOLO_V3_TINY_WEIGHTS        = \"model_data/yolov3-tiny.weights\"\n",
        "YOLO_V4_TINY_WEIGHTS        = \"model_data/yolov4-tiny.weights\"\n",
        "YOLO_TRT_QUANTIZE_MODE      = \"INT8\" # INT8, FP16, FP32\n",
        "YOLO_CUSTOM_WEIGHTS         = False # \"checkpoints/yolov3_custom\" # used in evaluate_mAP.py and custom model detection, if not using leave False\n",
        "                            # YOLO_CUSTOM_WEIGHTS also used with TensorRT and custom model detection\n",
        "YOLO_COCO_CLASSES           = \"model_data/coco/coco.names\"\n",
        "YOLO_STRIDES                = [8, 16, 32]\n",
        "YOLO_IOU_LOSS_THRESH        = 0.5\n",
        "YOLO_ANCHOR_PER_SCALE       = 3\n",
        "YOLO_MAX_BBOX_PER_SCALE     = 100\n",
        "YOLO_INPUT_SIZE             = 416\n",
        "if YOLO_TYPE                == \"yolov4\":\n",
        "    YOLO_ANCHORS            = [[[12,  16], [19,   36], [40,   28]],\n",
        "                               [[36,  75], [76,   55], [72,  146]],\n",
        "                               [[142,110], [192, 243], [459, 401]]]\n",
        "if YOLO_TYPE                == \"yolov3\":\n",
        "    YOLO_ANCHORS            = [[[10,  13], [16,   30], [33,   23]],\n",
        "                               [[30,  61], [62,   45], [59,  119]],\n",
        "                               [[116, 90], [156, 198], [373, 326]]]\n",
        "# Train options\n",
        "TRAIN_YOLO_TINY             = False\n",
        "TRAIN_SAVE_BEST_ONLY        = True # saves only best model according validation loss (True recommended)\n",
        "TRAIN_SAVE_CHECKPOINT       = False # saves all best validated checkpoints in training process (may require a lot disk space) (False recommended)\n",
        "TRAIN_CLASSES               = \"mnist/mnist.names\"\n",
        "TRAIN_ANNOT_PATH            = \"mnist/mnist_train.txt\"\n",
        "TRAIN_LOGDIR                = \"log\"\n",
        "TRAIN_CHECKPOINTS_FOLDER    = \"checkpoints\"\n",
        "TRAIN_MODEL_NAME            = f\"{YOLO_TYPE}_custom\"\n",
        "TRAIN_LOAD_IMAGES_TO_RAM    = True # With True faster training, but need more RAM\n",
        "TRAIN_BATCH_SIZE            = 4\n",
        "TRAIN_INPUT_SIZE            = 416\n",
        "TRAIN_DATA_AUG              = True\n",
        "TRAIN_TRANSFER              = True\n",
        "TRAIN_FROM_CHECKPOINT       = False # \"checkpoints/yolov3_custom\"\n",
        "TRAIN_LR_INIT               = 1e-4\n",
        "TRAIN_LR_END                = 1e-6\n",
        "TRAIN_WARMUP_EPOCHS         = 2\n",
        "TRAIN_EPOCHS                = 100\n",
        "\n",
        "# TEST options\n",
        "TEST_ANNOT_PATH             = \"mnist/mnist_test.txt\"\n",
        "TEST_BATCH_SIZE             = 4\n",
        "TEST_INPUT_SIZE             = 416\n",
        "TEST_DATA_AUG               = False\n",
        "TEST_DECTECTED_IMAGE_PATH   = \"\"\n",
        "TEST_SCORE_THRESHOLD        = 0.3\n",
        "TEST_IOU_THRESHOLD          = 0.45\n",
        "\n",
        "if TRAIN_YOLO_TINY:\n",
        "    YOLO_STRIDES            = [16, 32]    \n",
        "    # YOLO_ANCHORS            = [[[23, 27],  [37, 58],   [81,  82]], # this line can be uncommented for default coco weights\n",
        "    YOLO_ANCHORS            = [[[10, 14],  [23, 27],   [37, 58]],\n",
        "                               [[81,  82], [135, 169], [344, 319]]]"
      ],
      "metadata": {
        "id": "r_4p5NS1kp6j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Load_Yolo_model():\n",
        "        \n",
        "  if YOLO_FRAMEWORK == \"tf\": # TensorFlow detection\n",
        "      if YOLO_TYPE == \"yolov4\":\n",
        "          Darknet_weights = YOLO_V4_TINY_WEIGHTS if TRAIN_YOLO_TINY else YOLO_V4_WEIGHTS\n",
        "      if YOLO_TYPE == \"yolov3\":\n",
        "          Darknet_weights = YOLO_V3_TINY_WEIGHTS if TRAIN_YOLO_TINY else YOLO_V3_WEIGHTS\n",
        "          \n",
        "      if YOLO_CUSTOM_WEIGHTS == False:\n",
        "          print(\"Loading Darknet_weights from:\", Darknet_weights)\n",
        "          yolo = Create_Yolo(input_size=YOLO_INPUT_SIZE, CLASSES=YOLO_COCO_CLASSES)\n",
        "          load_yolo_weights(yolo, Darknet_weights) # use Darknet weights\n",
        "      else:\n",
        "          checkpoint = f\"./checkpoints/{TRAIN_MODEL_NAME}\"\n",
        "          if TRAIN_YOLO_TINY:\n",
        "              checkpoint += \"_Tiny\"\n",
        "          print(\"Loading custom weights from:\", checkpoint)\n",
        "          yolo = Create_Yolo(input_size=YOLO_INPUT_SIZE, CLASSES=TRAIN_CLASSES)\n",
        "          yolo.load_weights(checkpoint)  # use custom weights\n",
        "\n",
        "    return yolo"
      ],
      "metadata": {
        "id": "KFGAOGG7gOEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolo_weights(model, weights_file):\n",
        "    tf.keras.backend.clear_session() # used to reset layer names\n",
        "    # load Darknet original weights to TensorFlow model\n",
        "    if YOLO_TYPE == \"yolov3\":\n",
        "        range1 = 75 if not TRAIN_YOLO_TINY else 13\n",
        "        range2 = [58, 66, 74] if not TRAIN_YOLO_TINY else [9, 12]\n",
        "    if YOLO_TYPE == \"yolov4\":\n",
        "        range1 = 110 if not TRAIN_YOLO_TINY else 21\n",
        "        range2 = [93, 101, 109] if not TRAIN_YOLO_TINY else [17, 20]\n",
        "    \n",
        "    with open(weights_file, 'rb') as wf:\n",
        "        major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n",
        "\n",
        "        j = 0\n",
        "        for i in range(range1):\n",
        "            if i > 0:\n",
        "                conv_layer_name = 'conv2d_%d' %i\n",
        "            else:\n",
        "                conv_layer_name = 'conv2d'\n",
        "                \n",
        "            if j > 0:\n",
        "                bn_layer_name = 'batch_normalization_%d' %j\n",
        "            else:\n",
        "                bn_layer_name = 'batch_normalization'\n",
        "            \n",
        "            conv_layer = model.get_layer(conv_layer_name)\n",
        "            filters = conv_layer.filters\n",
        "            k_size = conv_layer.kernel_size[0]\n",
        "            in_dim = conv_layer.input_shape[-1]\n",
        "\n",
        "            if i not in range2:\n",
        "                # darknet weights: [beta, gamma, mean, variance]\n",
        "                bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n",
        "                # tf weights: [gamma, beta, mean, variance]\n",
        "                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n",
        "                bn_layer = model.get_layer(bn_layer_name)\n",
        "                j += 1\n",
        "            else:\n",
        "                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n",
        "\n",
        "            # darknet shape (out_dim, in_dim, height, width)\n",
        "            conv_shape = (filters, in_dim, k_size, k_size)\n",
        "            conv_weights = np.fromfile(wf, dtype=np.float32, count=np.product(conv_shape))\n",
        "            # tf shape (height, width, in_dim, out_dim)\n",
        "            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
        "\n",
        "            if i not in range2:\n",
        "                conv_layer.set_weights([conv_weights])\n",
        "                bn_layer.set_weights(bn_weights)\n",
        "            else:\n",
        "                conv_layer.set_weights([conv_weights, conv_bias])\n",
        "\n",
        "        assert len(wf.read()) == 0, 'failed to read all data'\n"
      ],
      "metadata": {
        "id": "Xq3lmswYkBwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolo = Load_Yolo_model()\n",
        "detect_image(yolo, image_path, \"./IMAGES/kite_pred.jpg\", input_size=YOLO_INPUT_SIZE, show=True, rectangle_colors=(255,0,0))"
      ],
      "metadata": {
        "id": "PZ982tWdfnZw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}