{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolo_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lOiSKVoqdqnA",
        "mi72zdD370_k"
      ],
      "mount_file_id": "1D1wCzZ-nXxk98Y-65JDByRo15VKdZH0m",
      "authorship_tag": "ABX9TyMljX7CJEQm7cNTt0QZqkms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/CV-Yolo/blob/main/Yolo_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W0qlttAELlC9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import PIL\n",
        "import colorsys\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.python.saved_model import tag_constants"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d aruchomu/data-for-yolo-v3-kernel\n",
        "!unzip \\*.zip && rm *.zip"
      ],
      "metadata": {
        "id": "hStuvGiBMSja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c2bbff-be2c-4497-d729-a51eb16b0610"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data-for-yolo-v3-kernel.zip to /content\n",
            " 96% 257M/267M [00:02<00:00, 117MB/s]\n",
            "100% 267M/267M [00:02<00:00, 116MB/s]\n",
            "Archive:  data-for-yolo-v3-kernel.zip\n",
            "  inflating: coco.names              \n",
            "  inflating: detections.gif          \n",
            "  inflating: dog.jpg                 \n",
            "  inflating: futur.ttf               \n",
            "  inflating: office.jpg              \n",
            "  inflating: yolov3.weights          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load The Yolo Model\n",
        "\n",
        "choose between yolo frameworks between; tf and trt\n",
        "\n",
        "\n",
        "and yolo types between;\n",
        "yolov3\n",
        "yolov3-tiny\n",
        "yolov4\n",
        "yolov4-tiny\n",
        "\n",
        "\n",
        " and decide if you need custom weights "
      ],
      "metadata": {
        "id": "kTSIBwjdMX5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolo_model(yolo_framework, yolo_type, yolo_costom_weights, input_size, input_classes, class_names):\n",
        "    \n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if yolo_framework == \"tf\": # TensorFlow detection framework\n",
        "        \n",
        "        if yolo_costom_weights:\n",
        "            checkpoint = f\"./checkpoints/{yolo_type}_custom\"\n",
        "            print(f\"Loading custom weights from: {checkpoint}\")\n",
        "            yolo = create_yolo_model(yolo_type, class_names, input_size=input_size, classes=input_classes)\n",
        "            yolo.load_weights(checkpoint)\n",
        "        else:\n",
        "            Darknet_weights = f'model_data/{yolo_type}.weights'\n",
        "            print(f\"Loading Darknet_weights from: {Darknet_weights}\")\n",
        "            yolo = create_yolo_model(yolo_type, class_names, input_size=input_size, classes=input_classes)\n",
        "            load_yolo_weights(yolo_type, yolo, Darknet_weights) # use Darknet weights\n",
        "        \n",
        "    elif yolo_framework == \"trt\": # TensorRT detection framework\n",
        "        saved_model_loaded = tf.saved_model.load(yolo_costom_weights, tags=[tag_constants.SERVING])\n",
        "        signature_keys = list(saved_model_loaded.signatures.keys())\n",
        "        yolo = saved_model_loaded.signatures['serving_default']\n",
        "\n",
        "    return yolo          "
      ],
      "metadata": {
        "id": "sG_4jEUNMZNh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load The Yolo Weights"
      ],
      "metadata": {
        "id": "XkP7insYr0Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_yolo_weights(yolo_type, model, weights_file):\n",
        "    tf.keras.backend.clear_session() # used to reset layer names\n",
        "    # load Darknet original weights to TensorFlow model\n",
        "    if yolo_type == \"yolov3\" or yolo_type[:-5] == \"yolov3\":\n",
        "        range1 = 75 if not yolo_type[-4:]=='tiny' else 13\n",
        "        range2 = [58, 66, 74] if not yolo_type[-4:]=='tiny' else [9, 12]\n",
        "    if yolo_type == \"yolov4\" or yolo_type[:-5] == \"yolov4\":\n",
        "        range1 = 110 if not yolo_type[-4:]=='tiny' else 21\n",
        "        range2 = [93, 101, 109] if not yolo_type[-4:]=='tiny' else [17, 20]\n",
        "    \n",
        "    with open(weights_file, 'rb') as wf:\n",
        "        major, minor, revision, seen, _ = np.fromfile(wf, dtype=np.int32, count=5)\n",
        "\n",
        "        j = 0\n",
        "        for i in range(range1):\n",
        "            conv_layer_name = 'conv2d' if i == 0 else f'conv2d_{i}'\n",
        "            bn_layer_name   = 'batch_normalization' if j == 0 else f'batch_normalization_{j}'\n",
        "            \n",
        "            conv_layer = model.get_layer(conv_layer_name)\n",
        "            \n",
        "            filters = conv_layer.filters\n",
        "            k_size = conv_layer.kernel_size[0]\n",
        "            in_dim = conv_layer.input_shape[-1]\n",
        "\n",
        "            if i not in range2:\n",
        "                # darknet weights: [beta, gamma, mean, variance]\n",
        "                bn_weights = np.fromfile(wf, dtype=np.float32, count=4 * filters)\n",
        "                # tf weights: [gamma, beta, mean, variance]\n",
        "                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n",
        "                bn_layer = model.get_layer(bn_layer_name)\n",
        "                j += 1\n",
        "            else:\n",
        "                conv_bias = np.fromfile(wf, dtype=np.float32, count=filters)\n",
        "\n",
        "            # darknet shape (out_dim, in_dim, height, width)\n",
        "            conv_shape = (filters, in_dim, k_size, k_size)\n",
        "            # filters * in_dim * k_size * k_size\n",
        "            conv_count = np.product(conv_shape)    \n",
        "            conv_weights = np.fromfile(wf, dtype=np.float32, count=conv_count)\n",
        "            # tf shape (height, width, in_dim, out_dim)\n",
        "            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n",
        "            # conv_weights = conv_weights.reshape(conv_shape[::-1])  # why this doesn't works?\n",
        "\n",
        "\n",
        "            if i not in range2:\n",
        "                conv_layer.set_weights([conv_weights])\n",
        "                bn_layer.set_weights(bn_weights)\n",
        "            else:\n",
        "                conv_layer.set_weights([conv_weights, conv_bias])\n",
        "\n",
        "        assert len(wf.read()) == 0, 'failed to read all data'\n"
      ],
      "metadata": {
        "id": "x_JmdmirrvrC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Yolo Model\n"
      ],
      "metadata": {
        "id": "3Boa4t2WNSS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_yolo_model(yolo_type, class_names, classes, input_size=416, channels=3, training=False):\n",
        "    \n",
        "    num_classes = len(class_names)\n",
        "    input_layer  = tf.keras.layers.Input([input_size, input_size, channels])  \n",
        "    if yolo_type[-4:] == 'tiny':\n",
        "        strides = np.array([16, 32])\n",
        "        yolo_anchors = [[[10, 14],  [23, 27],   [37, 58]],\n",
        "                        [[81,  82], [135, 169], [344, 319]]]\n",
        "\n",
        "        if yolo_type[:-5] == \"yolov4\":\n",
        "            convolutional_layers = YOLOv4_tiny(input_layer, num_classes)\n",
        "        if yolo_type[:-5] == \"yolov3\":\n",
        "            convolutional_layers = YOLOv3_tiny(input_layer, num_classes)\n",
        "    else:\n",
        "        strides = np.array([8, 16, 32])\n",
        "        if yolo_type == \"yolov4\":\n",
        "            yolo_anchors = [[[12,  16], [19,   36], [40,   28]],\n",
        "                            [[36,  75], [76,   55], [72,  146]],\n",
        "                            [[142,110], [192, 243], [459, 401]]]\n",
        "            convolutional_layers = YOLOv4(input_layer, num_classes)\n",
        "       \n",
        "        if yolo_type == \"yolov3\":\n",
        "            yolo_anchors  = [[[10,  13], [16,   30], [33,   23]],\n",
        "                             [[30,  61], [62,   45], [59,  119]],\n",
        "                             [[116, 90], [156, 198], [373, 326]]] \n",
        "            convolutional_layers = YOLOv3(input_layer, num_classes)\n",
        "\n",
        "\n",
        "    anchors = (np.array(yolo_anchors).T / strides).T\n",
        "    output_tensors = []\n",
        "    for i, conv_layer in enumerate(convolutional_layers):\n",
        "        pred_tensor = decode(conv_layer, num_classes, i, strides, anchors)\n",
        "\n",
        "        \n",
        "        if training: \n",
        "          output_tensors.append(conv_layer)\n",
        "        \n",
        "        output_tensors.append(pred_tensor)\n",
        "\n",
        "    Yolo = tf.keras.Model(input_layer, output_tensors)\n",
        "\n",
        "    return Yolo    "
      ],
      "metadata": {
        "id": "RR6_TxpcNVCp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Up Sample\n",
        "\n",
        "resize the batch of images' height and weidth\n",
        "\n",
        "    # shape=(None, 13, 13, 256)    ===>   (None, 26, 26, 256)\n",
        "    # shape=(None, 26, 26, 128)    ===>   (None, 56, 56, 128)"
      ],
      "metadata": {
        "id": "xV-iOWU0NXgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample(input_layer):\n",
        "    return tf.keras.layers.UpSampling2D(2)(input_layer)\n",
        "    # return tf.image.resize(input_layer, (input_layer.shape[1] * 2, input_layer.shape[2] * 2), method='nearest')"
      ],
      "metadata": {
        "id": "DTnP9K6zNXv2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Layer"
      ],
      "metadata": {
        "id": "YMFUMkkxNz33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolutional(input_layer, input_dim, output_dim, kernel_size, downsample=False, activate=True, bn=True, activate_type='leaky'):\n",
        "    if downsample:\n",
        "        input_layer = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 0)))(input_layer)\n",
        "        padding = 'valid'\n",
        "        strides = 2\n",
        "    else:\n",
        "        strides = 1\n",
        "        padding = 'same'\n",
        "\n",
        "    conv = tf.keras.layers.Conv2D(filters=output_dim,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  strides=strides,\n",
        "                                  padding=padding,\n",
        "                                  use_bias=not bn,\n",
        "                                  kernel_regularizer=tf.keras.regularizers.L2(0.0005),\n",
        "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),\n",
        "                                  bias_initializer=tf.constant_initializer(0.))(input_layer)  \n",
        "\n",
        "    if bn: # BatchNormalization\n",
        "        conv = tf.keras.layers.BatchNormalization()(conv)\n",
        "    if activate == True: # Activation\n",
        "        if activate_type == \"leaky\":\n",
        "            conv = tf.keras.layers.LeakyReLU(alpha=0.1)(conv)\n",
        "        elif activate_type == \"mish\":\n",
        "          conv = tf.math.softplus(conv)\n",
        "          conv = conv * tf.math.tanh(conv)\n",
        "\n",
        "    return conv "
      ],
      "metadata": {
        "id": "qOQfi4XqN2Tj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Residual Block\n",
        "\n",
        "this blocks uses 2 convolutional layers with different kernels and filters, but at last, their output's and input's dimention are same so we can concatenate them and prevent the model from loosing the details in lower layers.\n"
      ],
      "metadata": {
        "id": "kOPp6kF4N-iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, channels, filter1, filter2, activate_type='leaky'):\n",
        "    shortcut = x\n",
        "    x = convolutional(x, channels,filter1, 1, activate_type=activate_type)\n",
        "    x = convolutional(x, filter1, filter2, 3, activate_type=activate_type)\n",
        "\n",
        "    residual_layer = shortcut + x\n",
        "    return residual_layer"
      ],
      "metadata": {
        "id": "leAucGvIN_Tl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo V3"
      ],
      "metadata": {
        "id": "zlKh5ITE4Gpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P model_data https://pjreddie.com/media/files/yolov3.weights"
      ],
      "metadata": {
        "id": "SYO17crZ56a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a839eb1-f699-4321-9993-8e11b2aea751"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-21 06:03:52--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘model_data/yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M   103MB/s    in 2.3s    \n",
            "\n",
            "2022-08-21 06:03:55 (103 MB/s) - ‘model_data/yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DarkNet 53\n",
        "\n",
        "it returns 3 branches to the yolo model"
      ],
      "metadata": {
        "id": "q9CpBj_MIyRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def darknet53(input_data):\n",
        "    input_data = convolutional(input_data, 3, 32, 3)\n",
        "    input_data = convolutional(input_data, 32, 64, 3, downsample=True)\n",
        "\n",
        "    for i in range(1):\n",
        "        input_data = residual_block(input_data,  64, 32, 64)\n",
        "\n",
        "    input_data = convolutional(input_data, 64, 128, 3, downsample=True)\n",
        "\n",
        "    for i in range(2):\n",
        "        input_data = residual_block(input_data, 128, 64, 128)\n",
        "\n",
        "    input_data = convolutional(input_data, 128, 256, 3, downsample=True)\n",
        "\n",
        "    for i in range(8):\n",
        "        input_data = residual_block(input_data, 256, 128, 256)\n",
        "\n",
        "    route_1 = input_data\n",
        "    input_data = convolutional(input_data, 256, 512, 3, downsample=True)\n",
        "\n",
        "    for i in range(8):\n",
        "        input_data = residual_block(input_data, 512, 256, 512)\n",
        "\n",
        "    route_2 = input_data\n",
        "    input_data = convolutional(input_data, 512, 1024, 3, downsample=True)\n",
        "\n",
        "    for i in range(4):\n",
        "        input_data = residual_block(input_data, 1024, 512, 1024)\n",
        "\n",
        "    return route_1, route_2, input_data"
      ],
      "metadata": {
        "id": "sL8cXdFz4ODO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolov3 model\n",
        "\n",
        "it gets the results from the Darknet-53 bloack then predicts the pictures in 3 scales"
      ],
      "metadata": {
        "id": "fijLvIz9Jdn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def YOLOv3(input_layer, classes_count):\n",
        "    route_1, route_2, conv = darknet53(input_layer)\n",
        "\n",
        "    conv = convolutional(conv, 1024, 512, 1)\n",
        "    conv = convolutional(conv, 512, 1024, 3)\n",
        "    conv = convolutional(conv, 1024, 512, 1)\n",
        "    conv = convolutional(conv, 512, 1024, 3)\n",
        "    conv = convolutional(conv, 1024, 512, 1)\n",
        "    conv_lobj_branch = convolutional(conv, 512, 1024, 3)\n",
        "\n",
        "    # convolution_lbbox is used to predict large-sized objects , Shape = [None, 13, 13, 255]     \n",
        "    convolution_lbbox = convolutional(conv_lobj_branch, 1024, 3*(classes_count + 5), 1, activate_type=False, bn=False)\n",
        "\n",
        "    conv = convolutional(conv, 512,  256, 1)\n",
        "    # upsample here uses the \"nearest neighbor interpolation\" method, which has the advantage that the\n",
        "    # upsampling process does not need to learn, thereby reducing the network parameter  \n",
        "    conv = upsample(conv)\n",
        "\n",
        "    conv = tf.concat([conv, route_2], axis=-1)\n",
        "\n",
        "    conv = convolutional(conv, 768, 256, 1)\n",
        "    conv = convolutional(conv, 256, 512, 3)\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "    conv = convolutional(conv, 256, 512, 3)\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "    conv_mobj_branch = convolutional(conv, 256, 512, 3)\n",
        "\n",
        "    # convolution_mbbox is used to predict medium-sized objects, shape = [None, 26, 26, 255]\n",
        "    convolution_mbbox = convolutional(conv_mobj_branch, 512, 3*(classes_count + 5), 1, activate_type=False, bn=False)\n",
        "\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "    conv = upsample(conv)\n",
        "\n",
        "    conv = tf.concat([conv, route_1], axis=-1)\n",
        "    conv = convolutional(conv, 384, 128, 1)\n",
        "    conv = convolutional(conv, 128, 256, 3)\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "    conv = convolutional(conv, 128, 256, 3)\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "    conv_sobj_branch = convolutional(conv, 128, 256, 3)\n",
        "\n",
        "    # conv_sbbox is used to predict small size objects, shape = [None, 52, 52, 255]\n",
        "    conv_sbbox = convolutional(conv_sobj_branch, 256, 3*(classes_count +5), 1, activate_type=False, bn=False)\n",
        "        \n",
        "    return [conv_sbbox, convolution_mbbox, convolution_lbbox]"
      ],
      "metadata": {
        "id": "O6JopKdwIkOW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo V3 Tiny"
      ],
      "metadata": {
        "id": "lyuBaNNa5wte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P model_data https://pjreddie.com/media/files/yolov3-tiny.weights"
      ],
      "metadata": {
        "id": "IMwIg6125_E8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7784e67-09c1-4df4-b08d-3ad899d30426"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-21 06:03:55--  https://pjreddie.com/media/files/yolov3-tiny.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35434956 (34M) [application/octet-stream]\n",
            "Saving to: ‘model_data/yolov3-tiny.weights’\n",
            "\n",
            "yolov3-tiny.weights 100%[===================>]  33.79M  56.6MB/s    in 0.6s    \n",
            "\n",
            "2022-08-21 06:03:56 (56.6 MB/s) - ‘model_data/yolov3-tiny.weights’ saved [35434956/35434956]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DarkNet19_tiny"
      ],
      "metadata": {
        "id": "cHoTs16a6iVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def darknet19_tiny(input_data):\n",
        "    input_data = convolutional(input_data, 3, 16, 3)\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 2, 'same')(input_data)\n",
        "    input_data = convolutional(input_data, 16, 32, 3)\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 2, 'same')(input_data)\n",
        "    input_data = convolutional(input_data, 32, 64, 3)\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 2, 'same')(input_data)\n",
        "    input_data = convolutional(input_data, 64, 128, 3)\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 2, 'same')(input_data)\n",
        "    input_data = convolutional(input_data, 128, 256, 3)\n",
        "    route_1 = input_data\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 2, 'same')(input_data)\n",
        "    input_data = convolutional(input_data, 256, 512, 3)\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 1, 'same')(input_data)\n",
        "    input_data = convolutional(input_data, 512, 1024, 3)\n",
        "\n",
        "    return route_1, input_data"
      ],
      "metadata": {
        "id": "MaJocHKm5ziC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yolov3-Tiny model\n"
      ],
      "metadata": {
        "id": "R_wPFgyX7mQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def YOLOv3_tiny(input_layer, NUM_CLASS):\n",
        "    # After the input layer enters the Darknet-19 network, we get two branches\n",
        "    route_1, conv = darknet19_tiny(input_layer)\n",
        "\n",
        "    conv = convolutional(conv, 1024, 256, 1)\n",
        "    conv_lobj_branch = convolutional(conv, 256, 512, 3)\n",
        "    \n",
        "    # conv_lbbox is used to predict large-sized objects , Shape = [None, 26, 26, 255]\n",
        "    conv_lbbox = convolutional(conv_lobj_branch, 512, 3*(NUM_CLASS + 5), 1, activate_type=False, bn=False)\n",
        "\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "    # upsample here uses the nearest neighbor interpolation method, which has the advantage that the\n",
        "    # upsampling process does not need to learn, thereby reducing the network parameter  \n",
        "    conv = upsample(conv)\n",
        "    \n",
        "    conv = tf.concat([conv, route_1], axis=-1)\n",
        "    conv_mobj_branch = convolutional(conv, 128, 256, 3)\n",
        "    # conv_mbbox is used to predict medium size objects, shape = [None, 13, 13, 255]\n",
        "    conv_mbbox = convolutional(conv_mobj_branch, 256, 3 * (NUM_CLASS + 5), 1, activate_type=False, bn=False)\n",
        "\n",
        "    return [conv_mbbox, conv_lbbox]"
      ],
      "metadata": {
        "id": "QWqdgZl550B6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo V4"
      ],
      "metadata": {
        "id": "qmT-eAiV50Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P model_data https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights"
      ],
      "metadata": {
        "id": "-hPgvibk7p1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5795a5f-2f02-4ae9-b80e-5af7a577f39c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-21 06:03:56--  https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/75388965/ba4b6380-889c-11ea-9751-f994f5961796?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220821%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220821T060356Z&X-Amz-Expires=300&X-Amz-Signature=0607bc3c02192375793dffb89f44b11772204f9153209e22273da50fb70d5014&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=75388965&response-content-disposition=attachment%3B%20filename%3Dyolov4.weights&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-08-21 06:03:56--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/75388965/ba4b6380-889c-11ea-9751-f994f5961796?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220821%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220821T060356Z&X-Amz-Expires=300&X-Amz-Signature=0607bc3c02192375793dffb89f44b11772204f9153209e22273da50fb70d5014&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=75388965&response-content-disposition=attachment%3B%20filename%3Dyolov4.weights&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 257717640 (246M) [application/octet-stream]\n",
            "Saving to: ‘model_data/yolov4.weights’\n",
            "\n",
            "yolov4.weights      100%[===================>] 245.78M  9.22MB/s    in 20s     \n",
            "\n",
            "2022-08-21 06:04:17 (12.1 MB/s) - ‘model_data/yolov4.weights’ saved [257717640/257717640]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cspdarknet53(input_data):\n",
        "    input_data = convolutional(input_data, 3,  32, 3, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 32, 64, 3, downsample=True, activate_type=\"mish\")\n",
        "\n",
        "    route = input_data\n",
        "    route = convolutional(route, 64, 64, 1, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 64, 64, 1, activate_type=\"mish\")\n",
        "\n",
        "    for i in range(1):\n",
        "        input_data = residual_block(input_data, 64, 32, 64, activate_type=\"mish\")\n",
        "\n",
        "    input_data = convolutional(input_data, 64, 64, 1, activate_type=\"mish\")\n",
        "\n",
        "    input_data = tf.concat([input_data, route], axis=-1)\n",
        "    input_data = convolutional(input_data, 128, 64, 1, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 64, 128, 3, downsample=True, activate_type=\"mish\")\n",
        "    route = input_data\n",
        "\n",
        "    route = convolutional(route, 128, 64, 1, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 128, 64, 1, activate_type=\"mish\")\n",
        "\n",
        "    for i in range(2):\n",
        "        input_data = residual_block(input_data, 64,  64, 64, activate_type=\"mish\")\n",
        "\n",
        "    input_data = convolutional(input_data, 64, 64, 1, activate_type=\"mish\")\n",
        "    input_data = tf.concat([input_data, route], axis=-1)\n",
        "\n",
        "    input_data = convolutional(input_data, 128, 128, 1, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 128, 256, 3, downsample=True, activate_type=\"mish\")\n",
        "    route = input_data\n",
        "\n",
        "    route = convolutional(route, 256, 128, 1, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 256, 128, 1, activate_type=\"mish\")\n",
        "\n",
        "    for i in range(8):\n",
        "        input_data = residual_block(input_data, 128, 128, 128, activate_type=\"mish\")\n",
        "\n",
        "    input_data = convolutional(input_data, 128, 128, 1, activate_type=\"mish\")\n",
        "    input_data = tf.concat([input_data, route], axis=-1)\n",
        "\n",
        "    input_data = convolutional(input_data, 256, 256, 1, activate_type=\"mish\")\n",
        "    route_1 = input_data\n",
        "\n",
        "    input_data = convolutional(input_data, 256, 512, 3, downsample=True, activate_type=\"mish\")\n",
        "    route = input_data\n",
        "\n",
        "    route = convolutional(route, 512, 256, 1, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 512, 256, 1, activate_type=\"mish\")\n",
        "\n",
        "    for i in range(8):\n",
        "        input_data = residual_block(input_data, 256, 256, 256, activate_type=\"mish\")\n",
        "\n",
        "    input_data = convolutional(input_data, 256, 256, 1, activate_type=\"mish\")\n",
        "    input_data = tf.concat([input_data, route], axis=-1)\n",
        "\n",
        "    input_data = convolutional(input_data, 512, 512, 1, activate_type=\"mish\")\n",
        "    route_2 = input_data\n",
        "\n",
        "    input_data = convolutional(input_data, 512, 1024, 3, downsample=True, activate_type=\"mish\")\n",
        "    route = input_data\n",
        "\n",
        "    route = convolutional(route, 1024, 512, 1, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 1024, 512, 1, activate_type=\"mish\")\n",
        "\n",
        "    for i in range(4):\n",
        "        input_data = residual_block(input_data, 512, 512, 512, activate_type=\"mish\")\n",
        "        \n",
        "    input_data = convolutional(input_data, 512, 512, 1, activate_type=\"mish\")\n",
        "    input_data = tf.concat([input_data, route], axis=-1)\n",
        "\n",
        "    input_data = convolutional(input_data, 1024, 1024, 1, activate_type=\"mish\")\n",
        "    input_data = convolutional(input_data, 1024, 512, 1)\n",
        "    input_data = convolutional(input_data, 512, 1024, 3)\n",
        "    input_data = convolutional(input_data, 1024, 512, 1)\n",
        "\n",
        "    max_pooling_1 = tf.keras.layers.MaxPool2D(pool_size=13, padding='SAME', strides=1)(input_data)\n",
        "    max_pooling_2 = tf.keras.layers.MaxPool2D(pool_size=9, padding='SAME', strides=1)(input_data)\n",
        "    max_pooling_3 = tf.keras.layers.MaxPool2D(pool_size=5, padding='SAME', strides=1)(input_data)\n",
        "    input_data = tf.concat([max_pooling_1, max_pooling_2, max_pooling_3, input_data], axis=-1)\n",
        "\n",
        "    input_data = convolutional(input_data, 2048, 512, 1)\n",
        "    input_data = convolutional(input_data, 512, 1024, 3)\n",
        "    input_data = convolutional(input_data, 1024, 512, 1)\n",
        "\n",
        "    return route_1, route_2, input_data"
      ],
      "metadata": {
        "id": "qwrVRQMD7pw8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def YOLOv4(input_layer, NUM_CLASS):\n",
        "    route_1, route_2, conv = cspdarknet53(input_layer)\n",
        "\n",
        "    route = conv\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "    conv = upsample(conv)\n",
        "    route_2 = convolutional(route_2, 512, 256, 1)\n",
        "    conv = tf.concat([route_2, conv], axis=-1)\n",
        "\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "    conv = convolutional(conv, 256, 512, 1)\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "    conv = convolutional(conv, 256, 512, 1)\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "\n",
        "    route_2 = conv\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "    conv = upsample(conv)\n",
        "    route_1 = convolutional(route_1, 256, 128, 1)\n",
        "    conv = tf.concat([route_1, conv], axis=-1)\n",
        "\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "    conv = convolutional(conv, 128, 256, 3)\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "    conv = convolutional(conv, 128, 256, 3)\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "\n",
        "    route_1 = conv\n",
        "    conv = convolutional(conv, 128, 256, 3)\n",
        "    conv_sbbox = convolutional(conv, 256, 3 * (NUM_CLASS + 5), 1, activate=False, bn=False)\n",
        "\n",
        "    conv = convolutional(route_1, 128, 256, 3, downsample=True)\n",
        "    conv = tf.concat([conv, route_2], axis=-1)\n",
        "\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "    conv = convolutional(conv, 256, 512, 1)\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "    conv = convolutional(conv, 256, 512, 1)\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "\n",
        "    route_2 = conv\n",
        "    conv = convolutional(conv, 256, 512, 1)\n",
        "    conv_mbbox = convolutional(conv, 512, 3 * (NUM_CLASS + 5), 1, activate=False, bn=False)\n",
        "\n",
        "    conv = convolutional(route_2, 256, 512, 3, downsample=True)\n",
        "    conv = tf.concat([conv, route], axis=-1)\n",
        "\n",
        "    conv = convolutional(conv, 1024, 512, 1)\n",
        "    conv = convolutional(conv, 512, 1024, 3)\n",
        "    conv = convolutional(conv, 1024, 512, 1)\n",
        "    conv = convolutional(conv, 512, 1024, 3)\n",
        "    conv = convolutional(conv, 1024, 512, 1)\n",
        "\n",
        "    conv = convolutional(conv, 512, 1024, 3)\n",
        "    conv_lbbox = convolutional(conv, 1024, 3 * (NUM_CLASS + 5), 1, activate=False, bn=False)\n",
        "\n",
        "    return [conv_sbbox, conv_mbbox, conv_lbbox]"
      ],
      "metadata": {
        "id": "5Pi35psw7prl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg\n",
        "\n",
        "https://github.com/AlexeyAB/darknet\n",
        "\n",
        "\n",
        "https://github.com/AlexeyAB/darknet#yolo-v4-in-other-frameworks"
      ],
      "metadata": {
        "id": "9--ECzG-dBBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## https://github.com/AlexeyAB/darknet/wiki/YOLOv4-model-zoo\n",
        "\n",
        "\n",
        "## https://github.com/WongKinYiu/ScaledYOLOv4"
      ],
      "metadata": {
        "id": "lOiSKVoqdqnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AlexeyAB/darknet/wiki/Using-Yolo9000#using-yolo9000"
      ],
      "metadata": {
        "id": "dNsdvR7veIb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [net]\n",
        "# batch=64\n",
        "# subdivisions=8\n",
        "# # Training\n",
        "# #width=512\n",
        "# #height=512\n",
        "# width=608\n",
        "# height=608\n",
        "# channels=3\n",
        "# momentum=0.949\n",
        "# decay=0.0005\n",
        "# angle=0\n",
        "# saturation = 1.5\n",
        "# exposure = 1.5\n",
        "# hue=.1\n",
        "\n",
        "# learning_rate=0.0013\n",
        "# burn_in=1000\n",
        "# max_batches = 500500\n",
        "# policy=steps\n",
        "# steps=400000,450000\n",
        "# scales=.1,.1\n",
        "\n",
        "# #cutmix=1\n",
        "# mosaic=1\n",
        "\n",
        "# #:104x104 54:52x52 85:26x26 104:13x13 for 416\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=32\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# # Downsample\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=3\n",
        "# stride=2\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -2\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=32\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -1,-7\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# # Downsample\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=2\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -2\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=64\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -1,-10\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# # Downsample\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=2\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -2\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -1,-28\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# # Downsample\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=3\n",
        "# stride=2\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -2\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -1,-28\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# # Downsample\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=1024\n",
        "# size=3\n",
        "# stride=2\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -2\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [shortcut]\n",
        "# from=-3\n",
        "# activation=linear\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# [route]\n",
        "# layers = -1,-16\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=1024\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=mish\n",
        "\n",
        "# ##########################\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=1024\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# ### SPP ###\n",
        "# [maxpool]\n",
        "# stride=1\n",
        "# size=5\n",
        "\n",
        "# [route]\n",
        "# layers=-2\n",
        "\n",
        "# [maxpool]\n",
        "# stride=1\n",
        "# size=9\n",
        "\n",
        "# [route]\n",
        "# layers=-4\n",
        "\n",
        "# [maxpool]\n",
        "# stride=1\n",
        "# size=13\n",
        "\n",
        "# [route]\n",
        "# layers=-1,-3,-5,-6\n",
        "# ### End SPP ###\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=1024\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [upsample]\n",
        "# stride=2\n",
        "\n",
        "# [route]\n",
        "# layers = 85\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [route]\n",
        "# layers = -1, -3\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=512\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=512\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [upsample]\n",
        "# stride=2\n",
        "\n",
        "# [route]\n",
        "# layers = 54\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [route]\n",
        "# layers = -1, -3\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=256\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=256\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=128\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# ##########################\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=256\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=255\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [yolo]\n",
        "# mask = 0,1,2\n",
        "# anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401\n",
        "# classes=80\n",
        "# num=9\n",
        "# jitter=.3\n",
        "# ignore_thresh = .7\n",
        "# truth_thresh = 1\n",
        "# scale_x_y = 1.2\n",
        "# iou_thresh=0.213\n",
        "# cls_normalizer=1.0\n",
        "# iou_normalizer=0.07\n",
        "# iou_loss=ciou\n",
        "# nms_kind=greedynms\n",
        "# beta_nms=0.6\n",
        "# max_delta=5\n",
        "\n",
        "\n",
        "# [route]\n",
        "# layers = -4\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=2\n",
        "# pad=1\n",
        "# filters=256\n",
        "# activation=leaky\n",
        "\n",
        "# [route]\n",
        "# layers = -1, -16\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=512\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=512\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=256\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=512\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=255\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [yolo]\n",
        "# mask = 3,4,5\n",
        "# anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401\n",
        "# classes=80\n",
        "# num=9\n",
        "# jitter=.3\n",
        "# ignore_thresh = .7\n",
        "# truth_thresh = 1\n",
        "# scale_x_y = 1.1\n",
        "# iou_thresh=0.213\n",
        "# cls_normalizer=1.0\n",
        "# iou_normalizer=0.07\n",
        "# iou_loss=ciou\n",
        "# nms_kind=greedynms\n",
        "# beta_nms=0.6\n",
        "# max_delta=5\n",
        "\n",
        "\n",
        "# [route]\n",
        "# layers = -4\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=2\n",
        "# pad=1\n",
        "# filters=512\n",
        "# activation=leaky\n",
        "\n",
        "# [route]\n",
        "# layers = -1, -37\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=1024\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=1024\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# filters=512\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# batch_normalize=1\n",
        "# size=3\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=1024\n",
        "# activation=leaky\n",
        "\n",
        "# [convolutional]\n",
        "# size=1\n",
        "# stride=1\n",
        "# pad=1\n",
        "# filters=255\n",
        "# activation=linear\n",
        "\n",
        "\n",
        "# [yolo]\n",
        "# mask = 6,7,8\n",
        "# anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401\n",
        "# classes=80\n",
        "# num=9\n",
        "# jitter=.3\n",
        "# ignore_thresh = .7\n",
        "# truth_thresh = 1\n",
        "# random=1\n",
        "# scale_x_y = 1.05\n",
        "# iou_thresh=0.213\n",
        "# cls_normalizer=1.0\n",
        "# iou_normalizer=0.07\n",
        "# iou_loss=ciou\n",
        "# nms_kind=greedynms\n",
        "# beta_nms=0.6\n",
        "# max_delta=5\n"
      ],
      "metadata": {
        "id": "VaxATrTUc62l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo V4 Tiny"
      ],
      "metadata": {
        "id": "mi72zdD370_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P model_data https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights"
      ],
      "metadata": {
        "id": "OKFI_I3q737q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def route_group(input_layer, groups, group_id):\n",
        "    convs = tf.split(input_layer, num_or_size_splits=groups, axis=-1)\n",
        "    return convs[group_id]"
      ],
      "metadata": {
        "id": "prV1BsyL77o7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cspdarknet53_tiny(input_data): # not sure how this should be called\n",
        "    input_data = convolutional(input_data, 3, 32, 3, downsample=True)\n",
        "    input_data = convolutional(input_data, 32, 64, 3, downsample=True)\n",
        "    input_data = convolutional(input_data, 64, 64, 3)\n",
        "\n",
        "    route = input_data\n",
        "    input_data = route_group(input_data, 2, 1)\n",
        "    input_data = convolutional(input_data, 32, 32, 3)\n",
        "    route_1 = input_data\n",
        "    input_data = convolutional(input_data, 32, 32, 3)\n",
        "    input_data = tf.concat([input_data, route_1], axis=-1)\n",
        "    input_data = convolutional(input_data, 32, 64, 1)\n",
        "    input_data = tf.concat([route, input_data], axis=-1)\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 2, 'same')(input_data)\n",
        "\n",
        "    input_data = convolutional(input_data, 64, 128, 3)\n",
        "    route = input_data\n",
        "    input_data = route_group(input_data, 2, 1)\n",
        "    input_data = convolutional(input_data, 64, 64, 3)\n",
        "    route_1 = input_data\n",
        "    input_data = convolutional(input_data, 64, 64, 3)\n",
        "    input_data = tf.concat([input_data, route_1], axis=-1)\n",
        "    input_data = convolutional(input_data, 64, 128, 1)\n",
        "    input_data = tf.concat([route, input_data], axis=-1)\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 2, 'same')(input_data)\n",
        "\n",
        "    input_data = convolutional(input_data, 128, 256, 3)\n",
        "    route = input_data\n",
        "    input_data = route_group(input_data, 2, 1)\n",
        "    input_data = convolutional(input_data, 128, 128, 3)\n",
        "    route_1 = input_data\n",
        "    input_data = convolutional(input_data, 128, 128, 3)\n",
        "    input_data = tf.concat([input_data, route_1], axis=-1)\n",
        "    input_data = convolutional(input_data, 128, 256, 1)\n",
        "    route_1 = input_data\n",
        "    input_data = tf.concat([route, input_data], axis=-1)\n",
        "    input_data = tf.keras.layers.MaxPool2D(2, 2, 'same')(input_data)\n",
        "\n",
        "    input_data = convolutional(input_data, 512, 512, 3)\n",
        "\n",
        "    return route_1, input_data"
      ],
      "metadata": {
        "id": "jTYbj9Tx77mK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def YOLOv4_tiny(input_layer, NUM_CLASS):\n",
        "    route_1, conv = cspdarknet53_tiny(input_layer)\n",
        "\n",
        "    conv = convolutional(conv, 512, 256, 1)\n",
        "\n",
        "    conv_lobj_branch = convolutional(conv, 256, 512, 3)\n",
        "    conv_lbbox = convolutional(conv_lobj_branch, 1, 512, 3 * (NUM_CLASS + 5), 1, activate=False, bn=False)\n",
        "\n",
        "    conv = convolutional(conv, 256, 128, 1)\n",
        "    conv = upsample(conv)\n",
        "    conv = tf.concat([conv, route_1], axis=-1)\n",
        "\n",
        "    conv_mobj_branch = convolutional(conv, 128, 256, 3)\n",
        "    conv_mbbox = convolutional(conv_mobj_branch, 256, 3 * (NUM_CLASS + 5), 1, activate=False, bn=False)\n",
        "\n",
        "    return [conv_mbbox, conv_lbbox]"
      ],
      "metadata": {
        "id": "_UunTkiL78Dj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "DSgJtMxVF1an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(conv_output, classes_count, i, strides, anchors):\n",
        "\n",
        "    # where i = 0, 1 or 2 to correspond to the three grid scales  \n",
        "    conv_shape       = tf.shape(conv_output)\n",
        "    batch_size       = conv_shape[0]\n",
        "    output_size      = conv_shape[1]\n",
        "\n",
        "    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + classes_count))\n",
        "        # 0 [None, 52, 52, 255]  ===>  [None, 52, 52, 3, 5+classes_count]\n",
        "        # 1 [None, 26, 26, 255]  ===>  [None, 26, 26, 3, 5+classes_count]\n",
        "        # 2 [None, 13, 13, 255]  ===>  [None, 13, 13, 3, 5+classes_count]\n",
        "\n",
        "\n",
        "    conv_raw_dxdy, conv_raw_dwdh, conv_raw_conf, conv_raw_prob = tf.split(conv_output, (2, 2, 1, classes_count), axis=-1)\n",
        "        # 255 ===>2, 2, 1, 250\n",
        "        #conv_raw_dxdy = conv_output[:, :, :, :, 0:2] # offset of center position     \n",
        "        #conv_raw_dwdh = conv_output[:, :, :, :, 2:4] # Prediction box length and width offset\n",
        "        #conv_raw_conf = conv_output[:, :, :, :, 4:5] # confidence of the prediction box\n",
        "        #conv_raw_prob = conv_output[:, :, :, :, 5: ] # category probability of the prediction box\n",
        "\n",
        "\n",
        "    # next need Draw the grid. Where output_size is equal to 13, 26 or 52  \n",
        "    #y = tf.range(output_size, dtype=tf.int32)\n",
        "    #y = tf.expand_dims(y, -1)\n",
        "    #y = tf.tile(y, [1, output_size])\n",
        "    #x = tf.range(output_size,dtype=tf.int32)\n",
        "    #x = tf.expand_dims(x, 0)\n",
        "    #x = tf.tile(x, [output_size, 1])\n",
        "    xy_grid = tf.meshgrid(tf.range(output_size), tf.range(output_size))\n",
        "    xy_grid = tf.expand_dims(tf.stack(xy_grid, axis=-1), axis=2)  # [gx, gy, 1, 2]\n",
        "    xy_grid = tf.tile(tf.expand_dims(xy_grid, axis=0), [batch_size, 1, 1, 3, 1])\n",
        "    xy_grid = tf.cast(xy_grid, tf.float32)\n",
        "    \n",
        "    #xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n",
        "    #xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])\n",
        "    #y_grid = tf.cast(xy_grid, tf.float32)\n",
        "\n",
        "    # Calculate the center position of the prediction box:\n",
        "    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * strides[i]\n",
        "    # Calculate the length and width of the prediction box:\n",
        "    pred_wh = (tf.exp(conv_raw_dwdh) * anchors[i]) * strides[i]\n",
        "\n",
        "    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n",
        "    pred_conf = tf.sigmoid(conv_raw_conf) # object box calculates the predicted confidence\n",
        "    pred_prob = tf.sigmoid(conv_raw_prob) # calculating the predicted probability category box object\n",
        "\n",
        "    # calculating the predicted probability category box object\n",
        "    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)"
      ],
      "metadata": {
        "id": "xTqQry-YF3Fk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_image(Yolo, image_path, class_names,classes, input_size=416, show=False, score_threshold=0.3, iou_threshold=0.45, rectangle_colors='', yolo_framework='tf'):\n",
        "    \n",
        "    original_image      = cv2.imread(image_path)\n",
        "    original_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    original_image      = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    image_data = image_preprocess(np.copy(original_image), [input_size, input_size])\n",
        "    # image_data = image_data[np.newaxis, ...].astype(np.float32)\n",
        "    image_data = np.expand_dims(image_data, 0).astype(np.float32)\n",
        "\n",
        "\n",
        "    if yolo_framework == \"tf\":\n",
        "        pred_bbox = Yolo.predict(image_data)\n",
        "    elif yolo_framework == \"trt\":\n",
        "        batched_input = tf.constant(image_data)\n",
        "        result = Yolo(batched_input)\n",
        "        pred_bbox = []\n",
        "        for key, value in result.items():\n",
        "            value = value.numpy()\n",
        "            pred_bbox.append(value)\n",
        "    \n",
        "    pred_bbox = [tf.reshape(x, (-1, tf.shape(x)[-1])) for x in pred_bbox]\n",
        "    pred_bbox = tf.concat(pred_bbox, axis=0)\n",
        "    \n",
        "    bboxes = postprocess_boxes(pred_bbox, original_image, input_size, score_threshold)\n",
        "    bboxes = nms(bboxes, iou_threshold, method='nms')\n",
        "\n",
        "    image = draw_bbox(original_image, bboxes, class_names, classes=classes, rectangle_colors=rectangle_colors)\n",
        "    # CreateXMLfile(\"XML_Detections\", str(int(time.time())), original_image, bboxes, class_names)\n",
        "\n",
        "    output_path = f'{image_path[:-4]}_pred.jpg'\n",
        "    cv2.imwrite(output_path, image)\n",
        "    if show:\n",
        "        # Show the image\n",
        "        # cv2.imshow(\"predicted image\", image)\n",
        "        cv2_imshow(image)\n",
        "\n",
        "        # Load and hold the image\n",
        "        cv2.waitKey(0)\n",
        "        # To close the window after the required kill value was provided\n",
        "        cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Dh3_8gUWqO3N"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_preprocess(image, target_size, gt_boxes=None):\n",
        "    target_height, target_width  = target_size\n",
        "    h,  w, _  = image.shape\n",
        "\n",
        "    scale = min(target_width / w, target_height / h)\n",
        "    scaled_width, scaled_height  = int(scale * w), int(scale * h)\n",
        "\n",
        "    image_resized = cv2.resize(image, (scaled_width, scaled_height))\n",
        "    \n",
        "    image_paded = np.full(shape=[target_height, target_width, 3], fill_value=127.5)# fill_value = 255/2 == 127.5\n",
        "\n",
        "    offset_width, offset_height = (target_width - scaled_width) // 2, (target_height - scaled_height) // 2\n",
        "\n",
        "    image_paded[offset_height : scaled_height+offset_height, offset_width : scaled_width+offset_width, :] = image_resized\n",
        "    \n",
        "    image_paded = image_paded / 255.\n",
        "\n",
        "    if gt_boxes is None:\n",
        "        return image_paded\n",
        "\n",
        "    else:\n",
        "        gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]] * scale + offset_width\n",
        "        gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]] * scale + offset_height\n",
        "        return image_paded, gt_boxes"
      ],
      "metadata": {
        "id": "W7yKhsLTqTHt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_boxes(pred_bbox, original_image, input_size, score_threshold):\n",
        "    valid_scale=[0, np.inf]\n",
        "    pred_bbox = np.array(pred_bbox)\n",
        "\n",
        "    pred_xywh = pred_bbox[:, 0:4]\n",
        "    pred_conf = pred_bbox[:, 4]\n",
        "    pred_prob = pred_bbox[:, 5:]\n",
        "\n",
        "\n",
        "    # 1. (x, y, w, h) --> (xmin, ymin, xmax, ymax)\n",
        "    pred_coor = np.concatenate([pred_xywh[:, :2] - pred_xywh[:, 2:] * 0.5,\n",
        "                                pred_xywh[:, :2] + pred_xywh[:, 2:] * 0.5], axis=-1)\n",
        "    \n",
        "    \n",
        "    # 2. (xmin, ymin, xmax, ymax) -> (xmin_org, ymin_org, xmax_org, ymax_org)\n",
        "    org_h, org_w = original_image.shape[:2]\n",
        "    resize_ratio = min(input_size / org_w, input_size / org_h)\n",
        "\n",
        "    dw = (input_size - resize_ratio * org_w) / 2\n",
        "    dh = (input_size - resize_ratio * org_h) / 2\n",
        "\n",
        "#                                 pred_coor[:, 1::2] strart from 1 and jump indexis 2 by 2\n",
        "\n",
        "    pred_coor[:, 0::2] = (pred_coor[:, 0::2] - dw) / resize_ratio   #X, W\n",
        "    pred_coor[:, 1::2] = (pred_coor[:, 1::2] - dh) / resize_ratio   #Y, H\n",
        "    # 3. clip some boxes those are out of range\n",
        "    pred_coor = np.concatenate([np.maximum(pred_coor[:, :2], [0, 0]),\n",
        "                                np.minimum(pred_coor[:, 2:], [org_w - 1, org_h - 1])], axis=-1)\n",
        "    invalid_mask = np.logical_or((pred_coor[:, 0] > pred_coor[:, 2]), (pred_coor[:, 1] > pred_coor[:, 3]))  # x>w or y>h\n",
        "    pred_coor[invalid_mask] = 0\n",
        "\n",
        "    # 4. discard some invalid boxes\n",
        "    bboxes_scale = np.sqrt(np.multiply.reduce(pred_coor[:, 2:4] - pred_coor[:, 0:2], axis=-1))\n",
        "    scale_mask = np.logical_and((valid_scale[0] < bboxes_scale), (bboxes_scale < valid_scale[1]))\n",
        "\n",
        "    # 5. discard boxes with low scores\n",
        "    classes = np.argmax(pred_prob, axis=-1)\n",
        "    scores = pred_conf * pred_prob[np.arange(len(pred_coor)), classes]\n",
        "    # scores = pred_conf * pred_prob[:, classes]\n",
        "\n",
        "    score_mask = scores > score_threshold\n",
        "    mask = np.logical_and(scale_mask, score_mask)\n",
        "    coors, scores, classes = pred_coor[mask], scores[mask], classes[mask]\n",
        "    return np.concatenate([coors, scores[:, np.newaxis], classes[:, np.newaxis]], axis=-1)"
      ],
      "metadata": {
        "id": "ago8bUMXqWG0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_bbox(image, bboxes, class_names, classes, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors='', tracking=False):   \n",
        "    num_classes = len(class_names)\n",
        "    image_h, image_w, _ = image.shape\n",
        "    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(colors)\n",
        "    np.random.seed(None)\n",
        "\n",
        "    for i, bbox in enumerate(bboxes):\n",
        "        coor = np.array(bbox[:4], dtype=np.int32)\n",
        "        score = bbox[4]\n",
        "        class_ind = int(bbox[5])\n",
        "        bbox_color = rectangle_colors if rectangle_colors != '' else colors[class_ind]\n",
        "        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
        "        if bbox_thick < 1: bbox_thick = 1\n",
        "        fontScale = 0.75 * bbox_thick\n",
        "        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
        "\n",
        "        # put object rectangle\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n",
        "\n",
        "        if show_label:\n",
        "            # get text label\n",
        "            score_str = \" {:.3f}\".format(score) if show_confidence else \"\"\n",
        "\n",
        "            if tracking: score_str = \" \"+str(score)\n",
        "\n",
        "            try:\n",
        "                label = \"{}\".format(class_names[class_ind]) + score_str\n",
        "            except KeyError:\n",
        "                print(\"\"\"You received KeyError, this might be that you are trying to use yolo original weights,\n",
        "                while using custom classes, if using custom model in configs.py set YOLO_CUSTOM_WEIGHTS = True\"\"\")\n",
        "\n",
        "            # get text size\n",
        "            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "                                                                  fontScale, thickness=bbox_thick)\n",
        "            # put filled text rectangle\n",
        "            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n",
        "\n",
        "            # put text above rectangle\n",
        "            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
        "                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "y9sOqq93qW0l"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bboxes_iou(boxes1, boxes2):\n",
        "    boxes1 = np.array(boxes1)\n",
        "    boxes2 = np.array(boxes2)\n",
        "\n",
        "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
        "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
        "    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
        "    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
        "\n",
        "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
        "    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n",
        "    union_area    = boxes1_area + boxes2_area - inter_area\n",
        "    ious          = np.maximum(1.0 * inter_area / union_area, np.finfo(np.float32).eps)\n",
        "\n",
        "    return ious"
      ],
      "metadata": {
        "id": "-j20rVLxqab9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nms(bboxes, iou_threshold, sigma=0.3, method='nms'):\n",
        "    \"\"\"\n",
        "    :param bboxes: (xmin, ymin, xmax, ymax, score, class)\n",
        "    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf\n",
        "          https://github.com/bharatsingh430/soft-nms\n",
        "    \"\"\"\n",
        "    classes_in_img = list(set(bboxes[:, 5]))\n",
        "    best_bboxes = []\n",
        "    for cls in classes_in_img:\n",
        "        cls_mask = (bboxes[:, 5] == cls)\n",
        "        cls_bboxes = bboxes[cls_mask]\n",
        "        # Process 1: Determine whether the number of bounding boxes is greater than 0 \n",
        "        while len(cls_bboxes) > 0:\n",
        "            # Process 2: Select the bounding box with the highest score according to score order A\n",
        "            max_ind = np.argmax(cls_bboxes[:, 4])\n",
        "            best_bbox = cls_bboxes[max_ind]\n",
        "            best_bboxes.append(best_bbox)\n",
        "            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])\n",
        "            # Process 3: Calculate this bounding box A and\n",
        "            # Remain all iou of the bounding box and remove those bounding boxes whose iou value is higher than the threshold \n",
        "            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])\n",
        "            weight = np.ones((len(iou),), dtype=np.float32)\n",
        "\n",
        "            assert method in ['nms', 'soft-nms']\n",
        "\n",
        "            if method == 'nms':\n",
        "                iou_mask = iou > iou_threshold\n",
        "                weight[iou_mask] = 0.0\n",
        "\n",
        "            if method == 'soft-nms':\n",
        "                weight = np.exp(-(1.0 * iou ** 2 / sigma))\n",
        "\n",
        "            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight\n",
        "            score_mask = cls_bboxes[:, 4] > 0.\n",
        "            cls_bboxes = cls_bboxes[score_mask]\n",
        "\n",
        "    return best_bboxes"
      ],
      "metadata": {
        "id": "NrHBF19vqcmw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Boxes"
      ],
      "metadata": {
        "id": "KhduHHMu78Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# yolo_framework    ====> 'tf', 'trt'   \n",
        "# yolo_type         ====> yolov3, yolov4, yolov3-tiny, yolov4-tiny\n",
        "# input_classes     ====> 'mnist/mnist.names', '/content/coco.names'\n",
        "\n",
        "input_classes=\"/content/coco.names\"\n",
        "\n",
        "class_names = {}\n",
        "with open(input_classes, 'r') as data:\n",
        "    for ID, name in enumerate(data):\n",
        "        class_names[ID] = name.strip('\\n')\n",
        "\n",
        "yolo = load_yolo_model(yolo_framework='tf',\n",
        "                       yolo_type='yolov3',\n",
        "                       yolo_costom_weights=False,\n",
        "                       input_size=416,\n",
        "                       input_classes=input_classes, \n",
        "                       class_names=class_names)\n",
        "\n",
        "image_path = '/content/dog.jpg'\n",
        "image_path = '/content/office.jpg'\n",
        "\n",
        "\n",
        "detect_image(yolo, image_path, class_names=class_names, classes = input_classes ,input_size=416, show=True, rectangle_colors=(255,0,0))"
      ],
      "metadata": {
        "id": "0UdBcAxOOCyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# yolo_framework    ====> 'tf', 'trt'   \n",
        "# yolo_type         ====> yolov3, yolov4, yolov3-tiny, yolov4-tiny\n",
        "# input_classes     ====> 'mnist/mnist.names', '/content/coco.names'\n",
        "\n",
        "input_classes=\"/content/coco.names\"\n",
        "\n",
        "class_names = {}\n",
        "with open(input_classes, 'r') as data:\n",
        "    for ID, name in enumerate(data):\n",
        "        class_names[ID] = name.strip('\\n')\n",
        "\n",
        "yolo = load_yolo_model(yolo_framework='tf',\n",
        "                       yolo_type='yolov3',\n",
        "                       yolo_costom_weights=False,\n",
        "                       input_size=32*8,\n",
        "                       input_classes=input_classes, \n",
        "                       class_names=class_names)\n",
        "\n",
        "image_path = '/content/dog.jpg'\n",
        "image_path = '/content/office.jpg'\n",
        "\n",
        "\n",
        "detect_image(yolo, image_path, class_names=class_names, classes = input_classes ,input_size=32*8, show=True, rectangle_colors=(255,0,0))"
      ],
      "metadata": {
        "id": "sCxI7RojrS9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iep1IgKsCn9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}