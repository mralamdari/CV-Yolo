{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/YOLO/blob/main/Yolo_v8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVXK-JCZtAQZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz4D4hFnsI04"
      },
      "outputs": [],
      "source": [
        "#install necessary libraries\n",
        "!pip install awscli\n",
        "!pip install ultralytics\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV31ERFmWIWf"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IP4ViZFoHDr",
        "outputId": "028c36f1-c3af-4270-f933-0e7fef4587e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'OIDv4_ToolKit'...\n",
            "remote: Enumerating objects: 499, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 499 (delta 32), reused 52 (delta 31), pack-reused 444\u001b[K\n",
            "Receiving objects: 100% (499/499), 34.11 MiB | 35.00 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n"
          ]
        }
      ],
      "source": [
        "#clone OIDv4_ToolKit\n",
        "!git clone https://github.com/mralamdari/OIDv4_ToolKit.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1GTFM1JsmPL",
        "outputId": "af870ab9-3926-434d-cdcc-6253ca5fcab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/OIDv4_ToolKit\n",
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Truck.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the class-descriptions-boxable.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0mY\n",
            "...145%, 0 MB, 41785 KB/s, 0 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File class-descriptions-boxable.csv downloaded into OID/csv_folder/class-descriptions-boxable.csv.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the train-annotations-bbox.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0mY\n",
            "...100%, 1138 MB, 35822 KB/s, 32 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File train-annotations-bbox.csv downloaded into OID/csv_folder/train-annotations-bbox.csv.\u001b[0m\n",
            "\n",
            "\u001b[95mTruck\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 8078 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 200 images.\u001b[0m\n",
            "    [INFO] | Download of 200 images in train.\u001b[0m\n",
            "100% 200/200 [03:19<00:00,  1.00it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Truck of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Person.\u001b[0m\n",
            "\n",
            "\u001b[95mPerson\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 248384 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 200 images.\u001b[0m\n",
            "    [INFO] | Download of 200 images in train.\u001b[0m\n",
            "100% 200/200 [03:27<00:00,  1.04s/it]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Person of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Tree.\u001b[0m\n",
            "\n",
            "\u001b[95mTree\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 289999 online images for train.\u001b[0m\n",
            "    [INFO] | Limiting to 200 images.\u001b[0m\n",
            "    [INFO] | Download of 200 images in train.\u001b[0m\n",
            "100% 200/200 [03:15<00:00,  1.02it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Tree of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd OIDv4_ToolKit\n",
        "\n",
        "#Write your desired objects (--classes) and data type (--type_csv)  and the number of images you want (--limit)\n",
        "\n",
        "!python3 main.py downloader --classes Truck Person Tree --type_csv train --limit 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVX3Hk6ZALcj",
        "outputId": "79277255-65bb-43be-ba9f-54f9502252dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Currently in subdirectory: train\n",
            "Converting annotations for class: Tree\n",
            "100% 200/200 [00:24<00:00,  8.15it/s]\n",
            "Converting annotations for class: Person\n",
            "100% 200/200 [00:17<00:00, 11.14it/s]\n",
            "Converting annotations for class: Truck\n",
            "100% 200/200 [00:04<00:00, 40.28it/s]\n"
          ]
        }
      ],
      "source": [
        "!python mralamdari_annotations_converter.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRhn_kDuWeIV"
      },
      "source": [
        "#Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6lnPhayec-n"
      },
      "outputs": [],
      "source": [
        "path = '/content/data/obj/train/'\n",
        "os.makedirs('/content/train/images', exist_ok=True)\n",
        "os.makedirs('/content/train/labels', exist_ok=True)\n",
        "\n",
        "for j in os.listdir('/content/data/obj/train/'):\n",
        "  for i in os.listdir(path+j):\n",
        "    file_type = 'labels' if i[-4:] == '.txt' else 'images'\n",
        "    os.rename(path+j+'/'+i, f'/content/train/{file_type}/{i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYspNMCpec0a"
      },
      "outputs": [],
      "source": [
        "with open('/content/config.yaml', 'w+') as f:\n",
        "    config_files = f.write(f\"\"\"path: /content/train # dataset root dir\n",
        "train: images  # train images (relative to 'path')\n",
        "val: images  # val images (relative to 'path')\n",
        "\n",
        "\n",
        "# Classes\n",
        "names:\n",
        "  0: Truck\n",
        "  1: Person\n",
        "  2: Tree\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LliI1oaPRiTZ",
        "outputId": "be239436-d433-46d4-9370-21a5ccf0db8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Ultralytics YOLOv8.0.149 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/content/config.yaml, epochs=5, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
            "YOLOv8n summary: 225 layers, 3011433 parameters, 3011417 gradients\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/train/labels.cache... 586 images, 0 backgrounds, 0 corrupt: 100%|██████████| 586/586 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/train/labels.cache... 586 images, 0 backgrounds, 0 corrupt: 100%|██████████| 586/586 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/5         0G      1.463      2.817      1.533         51        640: 100%|██████████| 37/37 [08:23<00:00, 13.61s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [02:30<00:00,  7.94s/it]\n",
            "                   all        586       1695      0.653     0.0927      0.244      0.143\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/5         0G       1.52      2.303       1.59         51        640: 100%|██████████| 37/37 [08:18<00:00, 13.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [02:32<00:00,  8.00s/it]\n",
            "                   all        586       1695      0.406      0.293       0.26      0.136\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/5         0G      1.571       2.29      1.619         71        640: 100%|██████████| 37/37 [08:13<00:00, 13.34s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [02:32<00:00,  8.03s/it]\n",
            "                   all        586       1695      0.386      0.401      0.322       0.15\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        4/5         0G      1.573      2.169      1.591         43        640: 100%|██████████| 37/37 [08:14<00:00, 13.37s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [02:32<00:00,  8.04s/it]\n",
            "                   all        586       1695       0.43      0.439      0.395      0.226\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        5/5         0G      1.539      2.068      1.562         85        640:  73%|███████▎  | 27/37 [06:05<02:11, 13.12s/it]"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# model = YOLO('yolov8n.pt')\n",
        "model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n",
        "\n",
        "results = model.train(data='/content/config.yaml', epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFg9LcsPrNCS"
      },
      "outputs": [],
      "source": [
        "ggffdfdf\n",
        "sdkskds\n",
        "asldasldsl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYKroTzvALzs"
      },
      "outputs": [],
      "source": [
        "# os.makedirs(f'/content/data/train/images/', exist_ok=True)\n",
        "# os.makedirs(f'/content/data/train/labels/', exist_ok=True)\n",
        "# os.makedirs(f'/content/data/test/images/', exist_ok=True)\n",
        "# os.makedirs(f'/content/data/test/labels/', exist_ok=True)\n",
        "\n",
        "\n",
        "# path = '/content/data/obj/train/'\n",
        "# DATA_ROOT = '/content/data/'\n",
        "\n",
        "# VAL_THRESHOLD = 0.2\n",
        "\n",
        "# for category in os.listdir(path):\n",
        "#   category_path = path+category\n",
        "\n",
        "#   for i in os.listdir(category_path):\n",
        "#     if i[-4:] == '.txt':\n",
        "#       file_name = i[:-4]\n",
        "#       image_file = category_path + '/' + file_name + '.jpg'\n",
        "#       label_file = category_path + '/' + file_name + '.txt'\n",
        "#       rand_num = np.random.rand()\n",
        "#       target = 'test' if rand_num <= VAL_THRESHOLD else 'train'\n",
        "#       os.rename(image_file, f'/content/data/{target}/images/{file_name}.img')\n",
        "#       os.rename(label_file, f'/content/data/{target}/labels/{file_name}.tx')\n",
        "\n",
        "# os.rename('/content/data/obj', '/content/obj')\n",
        "# os.rename('/content/data/obj.names', '/content/obj.names')\n",
        "# os.rename('/content/data/train.txt', '/content/train.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXNW8w1yom2L"
      },
      "outputs": [],
      "source": [
        "# !scp -r runs '/content/drive/MyDrive/Computer Vision Projects/Yolo/'\n",
        "!scp -r runs '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6yLKQuY36pO",
        "outputId": "93a5f9c9-dc15-44cb-9298-28f19efc3149"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.149 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/train/labels.cache... 149 images, 0 backgrounds, 0 corrupt: 100%|██████████| 149/149 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:41<00:00,  4.18s/it]\n",
            "                   all        149        391      0.785      0.487      0.611      0.415\n",
            "                 Truck        149        199      0.823      0.257       0.44      0.271\n",
            "                Person        149        117      0.788      0.504      0.657      0.449\n",
            "                  Tree        149         75      0.745      0.701      0.736      0.527\n",
            "Speed: 1.5ms preprocess, 214.3ms inference, 0.0ms loss, 10.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "metrics = model.val()  # evaluate model performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwgTs4XyaM0W",
        "outputId": "39523112-16a2-4e51-867c-09310e81a43f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/2.jpg: 640x448 (no detections), 705.6ms\n",
            "Speed: 9.7ms preprocess, 705.6ms inference, 14.5ms postprocess per image at shape (1, 3, 640, 448)\n"
          ]
        }
      ],
      "source": [
        "results = model(\"/content/2.jpg\")  # predict on an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J8u3spzac-o",
        "outputId": "4de87e28-883e-4434-c9b0-27d66dd00ce2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.engine.results.Boxes object\n",
              " keypoints: None\n",
              " keys: ['boxes']\n",
              " masks: None\n",
              " names: {0: 'Truck', 1: 'Person', 2: 'Tree'}\n",
              " orig_img: array([[[218, 206, 194],\n",
              "         [218, 206, 194],\n",
              "         [218, 206, 194],\n",
              "         ...,\n",
              "         [220, 208, 197],\n",
              "         [220, 208, 197],\n",
              "         [220, 208, 197]],\n",
              " \n",
              "        [[218, 206, 194],\n",
              "         [218, 206, 194],\n",
              "         [218, 206, 194],\n",
              "         ...,\n",
              "         [220, 208, 197],\n",
              "         [220, 208, 197],\n",
              "         [220, 208, 197]],\n",
              " \n",
              "        [[218, 206, 194],\n",
              "         [218, 206, 194],\n",
              "         [218, 206, 194],\n",
              "         ...,\n",
              "         [220, 208, 197],\n",
              "         [220, 208, 197],\n",
              "         [220, 208, 197]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 86, 143, 177],\n",
              "         [104, 160, 195],\n",
              "         [ 97, 153, 188],\n",
              "         ...,\n",
              "         [141, 172, 201],\n",
              "         [141, 172, 201],\n",
              "         [151, 182, 211]],\n",
              " \n",
              "        [[ 91, 148, 182],\n",
              "         [113, 170, 204],\n",
              "         [ 99, 156, 190],\n",
              "         ...,\n",
              "         [148, 180, 206],\n",
              "         [148, 180, 206],\n",
              "         [137, 169, 196]],\n",
              " \n",
              "        [[ 99, 156, 190],\n",
              "         [104, 160, 195],\n",
              "         [ 84, 141, 175],\n",
              "         ...,\n",
              "         [145, 177, 204],\n",
              "         [145, 177, 204],\n",
              "         [148, 180, 206]]], dtype=uint8)\n",
              " orig_shape: (900, 600)\n",
              " path: '/content/2.jpg'\n",
              " probs: None\n",
              " save_dir: None\n",
              " speed: {'preprocess': 9.660482406616211, 'inference': 705.6317329406738, 'postprocess': 14.508724212646484}]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJbrQuNg36ye",
        "outputId": "f258e027-2876-4e75-e05c-0c434b97d652"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading https://ultralytics.com/images/bus.jpg to 'bus.jpg'...\n",
            "100%|██████████| 476k/476k [00:00<00:00, 10.0MB/s]\n",
            "image 1/1 /content/OIDv4_ToolKit/bus.jpg: 640x480 (no detections), 245.5ms\n",
            "Speed: 5.0ms preprocess, 245.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Ultralytics YOLOv8.0.149 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/detect/train/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 7, 8400) (5.9 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/14.6 MB 27.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.22.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (4.7.1)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 10.8s, installed 1 package: ['onnx>=1.12.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 13.1s, saved as 'runs/detect/train/weights/best.onnx' (11.7 MB)\n",
            "\n",
            "Export complete (14.8s)\n",
            "Results saved to \u001b[1m/content/OIDv4_ToolKit/runs/detect/train/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=runs/detect/train/weights/best.onnx imgsz=640 \n",
            "Validate:        yolo val task=detect model=runs/detect/train/weights/best.onnx imgsz=640 data=/content/config.yaml \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "path = model.export(format=\"onnx\")  # export the model to ONNX format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvC-ffzJI7ro"
      },
      "source": [
        "Pre trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vavmhWN7n9fH",
        "outputId": "55bf51d0-db2d-4b33-aaee-6633c05a06ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.148 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/content/drive/MyDrive/Computer Vision Projects/Yolo/runs/detect/train5/weights/best.pt, data=coco128.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "\n",
            "Dataset 'coco128.yaml' images not found ⚠️, missing path '/content/OIDv4_ToolKit/datasets/coco128/images/train2017'\n",
            "Downloading https://ultralytics.com/assets/coco128.zip to '/content/OIDv4_ToolKit/datasets/coco128.zip'...\n",
            "100%|██████████| 6.66M/6.66M [00:00<00:00, 74.8MB/s]\n",
            "Unzipping /content/OIDv4_ToolKit/datasets/coco128.zip to /content/OIDv4_ToolKit/datasets...\n",
            "Dataset download success ✅ (1.1s), saved to \u001b[1m/content/OIDv4_ToolKit/datasets\u001b[0m\n",
            "\n",
            "Overriding model.yaml nc=1 with nc=80\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/OIDv4_ToolKit/datasets/coco128/labels/train2017... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<00:00, 1472.57it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/OIDv4_ToolKit/datasets/coco128/labels/train2017.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/OIDv4_ToolKit/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3         0G      2.879      5.692      3.237        294        640: 100%|██████████| 8/8 [02:15<00:00, 16.96s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:38<00:00,  9.65s/it]\n",
            "                   all        128        929          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3         0G      2.921      5.711      3.259        251        640: 100%|██████████| 8/8 [02:03<00:00, 15.40s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:38<00:00,  9.62s/it]\n",
            "                   all        128        929          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3         0G      2.851      5.679      3.272        158        640: 100%|██████████| 8/8 [02:02<00:00, 15.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:45<00:00, 11.44s/it]\n",
            "                   all        128        929          0          0          0          0\n",
            "\n",
            "3 epochs completed in 0.142 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.148 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:37<00:00,  9.50s/it]\n",
            "                   all        128        929          0          0          0          0\n",
            "Speed: 7.1ms preprocess, 266.0ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "Ultralytics YOLOv8.0.148 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/OIDv4_ToolKit/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|██████████| 128/128 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 8/8 [00:32<00:00,  4.10s/it]\n",
            "                   all        128        929          0          0          0          0\n",
            "Speed: 2.3ms preprocess, 240.5ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Load a model\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"/content/drive/MyDrive/Computer Vision Projects/Yolo/runs/detect/train5/weights/best.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "model.train(data=\"coco128.yaml\", epochs=3)  # train the model\n",
        "metrics = model.val()  # evaluate model performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td6o_WG1k7Ew",
        "outputId": "b7d4e9c0-f051-40a2-d660-dee75be3cc62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
            "image 1/1 /content/OIDv4_ToolKit/bus.jpg: 640x480 (no detections), 333.9ms\n",
            "Speed: 5.7ms preprocess, 333.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Ultralytics YOLOv8.0.148 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs/detect/train/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.6s, saved as 'runs/detect/train/weights/best.onnx' (12.2 MB)\n",
            "\n",
            "Export complete (4.7s)\n",
            "Results saved to \u001b[1m/content/OIDv4_ToolKit/runs/detect/train/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=runs/detect/train/weights/best.onnx imgsz=640 \n",
            "Validate:        yolo val task=detect model=runs/detect/train/weights/best.onnx imgsz=640 data=None \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
        "path = model.export(format=\"onnx\")  # export the model to ONNX format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOVFVvV8WhFJ"
      },
      "source": [
        "#Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VAOs9QD1MNF"
      },
      "source": [
        "Adjust dataset for the Classification task only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j282RSwP1rOA"
      },
      "outputs": [],
      "source": [
        "labels = ['Tree', 'Person', 'Truck']\n",
        "\n",
        "for label in labels:\n",
        "  os.makedirs(f'/content/data/train/{label}', exist_ok=True)\n",
        "  os.makedirs(f'/content/data/train/{label}', exist_ok=True)\n",
        "\n",
        "  os.makedirs(f'/content/data/val/{label}', exist_ok=True)\n",
        "  os.makedirs(f'/content/data/val/{label}', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGCYhsE41L_p"
      },
      "outputs": [],
      "source": [
        "path = '/content/data/obj/train/'\n",
        "DATA_ROOT = '/content/data/'\n",
        "\n",
        "VAL_THRESHOLD = 0.2\n",
        "\n",
        "for category in os.listdir(path):\n",
        "  category_path = path+category\n",
        "\n",
        "  for i in os.listdir(category_path):\n",
        "    if i[-4:] == '.jpg':\n",
        "      file_name = i\n",
        "      image_file = category_path + '/' + i\n",
        "      rand_num = np.random.rand()\n",
        "      target = 'val' if rand_num <= VAL_THRESHOLD else 'train'\n",
        "      os.rename(image_file, f'/content/data/{target}/{category}/{i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08lDsq1n8Y2g"
      },
      "outputs": [],
      "source": [
        "os.rename('/content/data/obj', '/content/obj')\n",
        "os.rename('/content/data/obj.names', '/content/obj.names')\n",
        "os.rename('/content/data/train.txt', '/content/train.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyxsPf7JE8iO",
        "outputId": "7c054fb0-7b16-429f-d31d-47c4c6d67b98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.148 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=/content/data, epochs=20, patience=50, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train5\n",
            "Overriding model.yaml nc=1000 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n",
            "YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients\n",
            "Transferred 156/158 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train5', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.5, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.30000000000000004, 1.7], hue=[-0.015, 0.015]), Normalize(p=1.0, mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/classify/train5\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       1/20         0G     0.2753         16        224: 100%|██████████| 15/15 [00:17<00:00,  1.15s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
            "                   all        0.4          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       2/20         0G     0.2144         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.03s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n",
            "                   all      0.633          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       3/20         0G     0.1612         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.02s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
            "                   all      0.683          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       4/20         0G      0.114         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n",
            "                   all      0.733          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       5/20         0G    0.08431         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.04s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n",
            "                   all      0.767          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       6/20         0G    0.07614         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.02s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
            "                   all        0.8          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       7/20         0G    0.06811         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.02s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
            "                   all      0.767          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       8/20         0G    0.04843         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.02s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
            "                   all      0.783          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "       9/20         0G    0.02943         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]\n",
            "                   all      0.783          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      10/20         0G    0.02944         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n",
            "                   all      0.783          1\n",
            "Closing dataloader mosaic\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      11/20         0G    0.02771         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.05s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\n",
            "                   all       0.75          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      12/20         0G    0.02305         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.03s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n",
            "                   all      0.767          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      13/20         0G    0.01671         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
            "                   all      0.767          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      14/20         0G    0.02172         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
            "                   all       0.75          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      15/20         0G    0.02428         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
            "                   all      0.783          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      16/20         0G    0.03106         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
            "                   all       0.75          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      17/20         0G    0.02208         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:02<00:00,  1.33s/it]\n",
            "                   all      0.767          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      18/20         0G    0.01562         16        224: 100%|██████████| 15/15 [00:18<00:00,  1.24s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n",
            "                   all       0.75          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      19/20         0G    0.01631         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
            "                   all       0.75          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "      20/20         0G    0.01831         16        224: 100%|██████████| 15/15 [00:15<00:00,  1.01s/it]\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n",
            "                   all       0.75          1\n",
            "\n",
            "20 epochs completed in 0.099 hours.\n",
            "Optimizer stripped from runs/classify/train5/weights/last.pt, 3.0MB\n",
            "Optimizer stripped from runs/classify/train5/weights/best.pt, 3.0MB\n",
            "Results saved to \u001b[1mruns/classify/train5\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n-cls.pt')\n",
        "\n",
        "results = model.train(data='/content/data', epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrMpsXWYZcc2"
      },
      "outputs": [],
      "source": [
        "!scp -r runs '/content/drive/MyDrive/Computer Vision Projects/Yolo/'\n",
        "!scp -r runs '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBW6tEfoZARR",
        "outputId": "8b6a24e3-7276-4489-bea7-6cb781402e2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.0.148 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon 2.20GHz)\n",
            "               classes   top1_acc   top5_acc: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]\n",
            "                   all        0.8          1\n",
            "Speed: 0.0ms preprocess, 16.0ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/classify/val2\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.800000011920929"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics = model.val()\n",
        "metrics.top1   # top1 accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVrkZgOKRjvh",
        "outputId": "67234882-161d-4f7c-a982-6d9cbebbaffd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics.top5   # top5 accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8haxuQXRuyo",
        "outputId": "732ff60b-0136-4601-af05-571cf9cadeb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /content/blue-truck-homepage-618x340.jpg: 224x224 Truck 1.00, Person 0.00, Tree 0.00, 30.6ms\n",
            "Speed: 1.8ms preprocess, 30.6ms inference, 0.1ms postprocess per image at shape (1, 3, 224, 224)\n"
          ]
        }
      ],
      "source": [
        "results = model('/content/blue-truck-homepage-618x340.jpg')  # predict on an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owNnEPXdRzVV",
        "outputId": "5b33f065-28d5-443d-d086-d540c82ebe41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: None\n",
              " keypoints: None\n",
              " keys: ['probs']\n",
              " masks: None\n",
              " names: {0: 'Person', 1: 'Tree', 2: 'Truck'}\n",
              " orig_img: array([[[213, 172, 140],\n",
              "         [211, 170, 138],\n",
              "         [208, 167, 135],\n",
              "         ...,\n",
              "         [223, 203, 186],\n",
              "         [225, 206, 191],\n",
              "         [228, 209, 194]],\n",
              " \n",
              "        [[213, 172, 140],\n",
              "         [210, 169, 137],\n",
              "         [207, 166, 134],\n",
              "         ...,\n",
              "         [225, 205, 188],\n",
              "         [227, 208, 193],\n",
              "         [229, 210, 195]],\n",
              " \n",
              "        [[211, 170, 138],\n",
              "         [209, 168, 136],\n",
              "         [206, 165, 133],\n",
              "         ...,\n",
              "         [229, 209, 192],\n",
              "         [228, 209, 194],\n",
              "         [231, 212, 197]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[186, 197, 205],\n",
              "         [181, 192, 200],\n",
              "         [179, 190, 198],\n",
              "         ...,\n",
              "         [200, 209, 212],\n",
              "         [198, 207, 210],\n",
              "         [200, 209, 212]],\n",
              " \n",
              "        [[186, 197, 205],\n",
              "         [181, 192, 200],\n",
              "         [180, 191, 199],\n",
              "         ...,\n",
              "         [198, 207, 210],\n",
              "         [196, 205, 208],\n",
              "         [199, 208, 211]],\n",
              " \n",
              "        [[187, 198, 206],\n",
              "         [182, 193, 201],\n",
              "         [180, 191, 199],\n",
              "         ...,\n",
              "         [198, 207, 210],\n",
              "         [201, 210, 213],\n",
              "         [203, 212, 215]]], dtype=uint8)\n",
              " orig_shape: (340, 618)\n",
              " path: '/content/blue-truck-homepage-618x340.jpg'\n",
              " probs: ultralytics.engine.results.Probs object\n",
              " save_dir: None\n",
              " speed: {'preprocess': 1.8031597137451172, 'inference': 30.628204345703125, 'postprocess': 0.09059906005859375}]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU5b0mtsR3Xw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "res = plt.imread('/content/blue-truck-homepage-618x340.jpg')\n",
        "plt.imshow(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TREwcC7yZgjk"
      },
      "source": [
        "#va"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZmslT9t0VaN"
      },
      "outputs": [],
      "source": [
        "# !pip install fiftyone\n",
        "\n",
        "# import fiftyone.zoo as foz\n",
        "\n",
        "# oi_dataset = foz.load_zoo_dataset(\"open-images-v6\", split=\"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLtU7OB1yGAN"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/openimages/v6/oidv6-class-descriptions.csv\n",
        "\n",
        "!wget https://storage.googleapis.com/openimages/v6/oidv6-train-images-with-labels-with-rotation.csv\n",
        "!wget https://storage.googleapis.com/openimages/2018_04/validation/validation-images-with-rotation.csv\n",
        "!wget https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv\n",
        "\n",
        "!wget https://raw.githubusercontent.com/openimages/dataset/master/downloader.py\n",
        "\n",
        "!wget https://raw.githubusercontent.com/drbillah/openimages/main/image_list.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHi4RDifWXr2"
      },
      "outputs": [],
      "source": [
        "!python /content/image_list.py -c Cattle,Sheep,Goat -d /content/oidv6-class-descriptions.csv -p /content/oidv6-train-images-with-labels-with-rotation.csv\n",
        "# !python /content/image_list.py -c Cattle,Sheep,Goat -d /content/oidv6-class-descriptions.csv -p /content/validation-images-with-rotation.csv\n",
        "# !python /content/image_list.py -c Cattle,Sheep,Goat -d /content/oidv6-class-descriptions.csv -p /content/test-images-with-rotation.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSn9D2E2scU-"
      },
      "outputs": [],
      "source": [
        "!python downloader.py <IMAGE_LIST_FILE> --download_folder <DOWNLOAD_FOLDER> --num_processes <number>\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help           show this help message and exit\n",
        "  --num_processes      Number of parallel processes to use (default is 5).\n",
        "  --download_folder    Folder where to download the images.\n",
        "\n",
        "  Example:\n",
        "python downloader.py train.txt --download_folder /home/lab/myfolder/ --num_processes 5\n",
        "python downloader.py test.txt --download_folder /home/lab/myfolder/ --num_processes 5\n",
        "python downloader.py validation.txt --download_folder /home/lab/myfolder/ --num_processes 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncf37FuloVy8"
      },
      "source": [
        "#ddd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YgGctLUSoXXj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage import draw\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "def create_image(path, img_size, min_radius):\n",
        "    path.parent.mkdir( parents=True, exist_ok=True )\n",
        "\n",
        "    arr = np.zeros((img_size, img_size)).astype(np.uint8)\n",
        "    center_x = random.randint(min_radius, (img_size-min_radius))\n",
        "    center_y = random.randint(min_radius, (img_size-min_radius))\n",
        "    max_radius = min(center_x, center_y, img_size - center_x, img_size - center_y)\n",
        "    radius = random.randint(min_radius, max_radius)\n",
        "\n",
        "    row_indxs, column_idxs = draw.ellipse(center_x, center_y, radius, radius, shape=arr.shape)\n",
        "\n",
        "    arr[row_indxs, column_idxs] = 255\n",
        "\n",
        "    im = Image.fromarray(arr)\n",
        "    im.save(path)\n",
        "\n",
        "def create_images(data_root_path, train_num, val_num, test_num, img_size=640, min_radius=10):\n",
        "    data_root_path = Path(data_root_path)\n",
        "\n",
        "    for i in range(train_num):\n",
        "        create_image(data_root_path / 'train' / 'images' / f'img_{i}.png', img_size, min_radius)\n",
        "\n",
        "    for i in range(val_num):\n",
        "        create_image(data_root_path / 'val' / 'images' / f'img_{i}.png', img_size, min_radius)\n",
        "\n",
        "    for i in range(test_num):\n",
        "        create_image(data_root_path / 'test' / 'images' / f'img_{i}.png', img_size, min_radius)\n",
        "\n",
        "create_images('datasets', train_num=120, val_num=40, test_num=40, img_size=120, min_radius=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/datasets/train/images/img_100.png'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(plt.imread(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "_8KZumHB3KE3",
        "outputId": "6997a23a-f9c4-4e30-f5d9-3b485b6c86c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79e33591b9a0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvFySufq_YwE",
        "outputId": "6a0cc375-45d0-4591-d344-e9eefc0fcb0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2023.7.22)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.23.5)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n",
            "Installing collected packages: snuggs, affine, rasterio\n",
            "Successfully installed affine-2.4.0 rasterio-1.3.8 snuggs-1.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rasterio import features\n",
        "\n",
        "def create_label(image_path, label_path):\n",
        "    arr = np.asarray(Image.open(image_path))\n",
        "\n",
        "    # There may be a better way to do it, but this is what I have found so far\n",
        "    cords = list(features.shapes(arr, mask=(arr >0)))[0][0]['coordinates'][0]\n",
        "    label_line = '0 ' + ' '.join([f'{int(cord[0])/arr.shape[0]} {int(cord[1])/arr.shape[1]}' for cord in cords])\n",
        "\n",
        "    label_path.parent.mkdir( parents=True, exist_ok=True )\n",
        "    with label_path.open('w') as f:\n",
        "        f.write(label_line)\n",
        "\n",
        "for images_dir_path in [Path(f'datasets/{x}/images') for x in ['train', 'val', 'test']]:\n",
        "    for img_path in images_dir_path.iterdir():\n",
        "        label_path = img_path.parent.parent / 'labels' / f'{img_path.stem}.txt'\n",
        "        label_line = create_label(img_path, label_path)"
      ],
      "metadata": {
        "id": "NeiCXjIO6Odv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_content = f'''\n",
        "train: train/images\n",
        "val: val/images\n",
        "test: test/images\n",
        "\n",
        "names: ['circle']\n",
        "    '''\n",
        "\n",
        "with Path('data.yaml').open('w') as f:\n",
        "    f.write(yaml_content)"
      ],
      "metadata": {
        "id": "Cf3dK8Il_VhU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics==8.0.38"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXaHzGfoIxHf",
        "outputId": "ec58dfff-1c5e-4446-8b9b-b112148c2935"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics==8.0.38\n",
            "  Downloading ultralytics-8.0.38-py3-none-any.whl (278 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (4.66.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (2.12.3)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (5.9.5)\n",
            "Collecting thop>=0.1.1 (from ultralytics==8.0.38)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: wheel>=0.38.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.38) (0.41.1)\n",
            "Collecting sentry-sdk (from ultralytics==8.0.38)\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.38) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.38) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.38) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.38) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.38) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.38) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.38) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.0.38) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.38) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.38) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.38) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.38) (2023.7.22)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.4.1->ultralytics==8.0.38) (2.3.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.38) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.38) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.38) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.38) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.38) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics==8.0.38) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics==8.0.38) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics==8.0.38) (16.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.38) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.38) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.38) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.38) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->ultralytics==8.0.38) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->ultralytics==8.0.38) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->ultralytics==8.0.38) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->ultralytics==8.0.38) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.4.1->ultralytics==8.0.38) (3.2.2)\n",
            "Installing collected packages: sentry-sdk, thop, ultralytics\n",
            "Successfully installed sentry-sdk-1.29.2 thop-0.1.1.post2209072238 ultralytics-8.0.38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8n-seg.pt\")\n",
        "\n",
        "results = model.train(\n",
        "        batch=8,\n",
        "        device=\"cpu\",\n",
        "        data=\"data.yaml\",\n",
        "        epochs=7,\n",
        "        imgsz=120,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nku28QBmBcHi",
        "outputId": "e271da0d-6f2a-42f4-904a-a96781aca817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt to yolov8n-seg.pt...\n",
            "100%|██████████| 6.73M/6.73M [00:00<00:00, 168MB/s]\n",
            "Ultralytics YOLOv8.0.38 🚀 Python-3.10.12 torch-2.0.1+cu118 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.yaml, data=data.yaml, epochs=7, patience=50, batch=8, imgsz=120, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, min_memory=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, split=val, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, save_dir=runs/segment/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100%|██████████| 755k/755k [00:00<00:00, 38.3MB/s]\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.Segment               [1, 32, 64, [64, 128, 256]]   \n",
            "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients, 12.1 GFLOPs\n",
            "\n",
            "Transferred 381/417 items from pretrained weights\n",
            "WARNING ⚠️ imgsz=[120] must be multiple of max stride 32, updating to [128]\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.001), 76 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/train/labels... 120 images, 0 backgrounds, 0 corrupt: 100%|██████████| 120/120 [00:00<00:00, 1624.22it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/val/labels... 40 images, 0 backgrounds, 0 corrupt: 100%|██████████| 40/40 [00:00<00:00, 1417.68it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/val/labels.cache\n",
            "Image sizes 128 train, 128 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/segment/train\u001b[0m\n",
            "Starting training for 7 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/7         0G      1.087      1.063      3.423     0.9503         13        128: 100%|██████████| 15/15 [00:09<00:00,  1.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:02<00:00,  1.44it/s]\n",
            "                   all         40         40     0.0347       0.95     0.0937      0.079     0.0366          1     0.0951     0.0752\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/7         0G     0.6278     0.5152      2.961     0.8826         11        128: 100%|██████████| 15/15 [00:07<00:00,  2.01it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n",
            "                   all         40         40    0.00522          1      0.192      0.166    0.00522          1      0.194      0.157\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/7         0G     0.6825       0.41      2.117     0.8678         12        128: 100%|██████████| 15/15 [00:09<00:00,  1.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n",
            "                   all         40         40     0.0046          1        0.2      0.173     0.0046          1        0.2      0.156\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        4/7         0G      0.787     0.3998      1.496     0.8715         16        128:  40%|████      | 6/15 [00:07<00:09,  1.06s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image as show_image\n",
        "show_image(filename=\"runs/segment/train60/val_batch0_labels.jpg\")"
      ],
      "metadata": {
        "id": "ED0WbvnbAK-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CV31ERFmWIWf",
        "PRhn_kDuWeIV",
        "DOVFVvV8WhFJ"
      ],
      "provenance": [],
      "mount_file_id": "1UgnJpOsz1gkeMcHt9NzdSkiGXMra3Bg8",
      "authorship_tag": "ABX9TyN+e0av24cMHckhUjT7D+HY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}