{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO_v3_ObjectDetection_TensorFlow_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1gCcO2AQlQ7yPjhManytzBubME16kuSDj",
      "authorship_tag": "ABX9TyM2hmb8hl7HrtLl5h/Sw88p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/CV-Yolo/blob/main/YOLO_v3_ObjectDetection_TensorFlow_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4IM6XF3D_oZe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d aruchomu/data-for-yolo-v3-kernel\n",
        "!unzip \\*.zip && rm *.zip"
      ],
      "metadata": {
        "id": "jcIsA7X-qOj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8b1d9b-a8c6-4e5f-c4b3-fb2399b6a736"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data-for-yolo-v3-kernel.zip to /content\n",
            " 94% 251M/267M [00:10<00:00, 20.3MB/s]\n",
            "100% 267M/267M [00:10<00:00, 26.9MB/s]\n",
            "Archive:  data-for-yolo-v3-kernel.zip\n",
            "  inflating: coco.names              \n",
            "  inflating: detections.gif          \n",
            "  inflating: dog.jpg                 \n",
            "  inflating: futur.ttf               \n",
            "  inflating: office.jpg              \n",
            "  inflating: yolov3.weights          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOLO options\n",
        "YOLO_TYPE                   = \"yolov3\" # yolov4 or yolov3\n",
        "YOLO_FRAMEWORK              = \"tf\" # \"tf\" or \"trt\"\n",
        "YOLO_V3_WEIGHTS             = \"model_data/yolov3.weights\"\n",
        "YOLO_V4_WEIGHTS             = \"model_data/yolov4.weights\"\n",
        "YOLO_V3_TINY_WEIGHTS        = \"model_data/yolov3-tiny.weights\"\n",
        "YOLO_V4_TINY_WEIGHTS        = \"model_data/yolov4-tiny.weights\"\n",
        "YOLO_TRT_QUANTIZE_MODE      = \"INT8\" # INT8, FP16, FP32\n",
        "YOLO_CUSTOM_WEIGHTS         = False # \"checkpoints/yolov3_custom\" # used in evaluate_mAP.py and custom model detection, if not using leave False\n",
        "                            # YOLO_CUSTOM_WEIGHTS also used with TensorRT and custom model detection\n",
        "YOLO_COCO_CLASSES           = \"model_data/coco/coco.names\"\n",
        "YOLO_STRIDES                = [8, 16, 32]\n",
        "YOLO_IOU_LOSS_THRESH        = 0.5\n",
        "YOLO_ANCHOR_PER_SCALE       = 3\n",
        "YOLO_MAX_BBOX_PER_SCALE     = 100\n",
        "YOLO_INPUT_SIZE             = 416\n",
        "if YOLO_TYPE                == \"yolov4\":\n",
        "    YOLO_ANCHORS            = [[[12,  16], [19,   36], [40,   28]],\n",
        "                               [[36,  75], [76,   55], [72,  146]],\n",
        "                               [[142,110], [192, 243], [459, 401]]]\n",
        "if YOLO_TYPE                == \"yolov3\":\n",
        "    YOLO_ANCHORS            = [[[10,  13], [16,   30], [33,   23]],\n",
        "                               [[30,  61], [62,   45], [59,  119]],\n",
        "                               [[116, 90], [156, 198], [373, 326]]]\n",
        "# Train options\n",
        "TRAIN_YOLO_TINY             = False\n",
        "TRAIN_SAVE_BEST_ONLY        = True # saves only best model according validation loss (True recommended)\n",
        "TRAIN_SAVE_CHECKPOINT       = False # saves all best validated checkpoints in training process (may require a lot disk space) (False recommended)\n",
        "TRAIN_CLASSES               = \"mnist/mnist.names\"\n",
        "TRAIN_ANNOT_PATH            = \"mnist/mnist_train.txt\"\n",
        "TRAIN_LOGDIR                = \"log\"\n",
        "TRAIN_CHECKPOINTS_FOLDER    = \"checkpoints\"\n",
        "TRAIN_MODEL_NAME            = f\"{YOLO_TYPE}_custom\"\n",
        "TRAIN_LOAD_IMAGES_TO_RAM    = True # With True faster training, but need more RAM\n",
        "TRAIN_BATCH_SIZE            = 4\n",
        "TRAIN_INPUT_SIZE            = 416\n",
        "TRAIN_DATA_AUG              = True\n",
        "TRAIN_TRANSFER              = True\n",
        "TRAIN_FROM_CHECKPOINT       = False # \"checkpoints/yolov3_custom\"\n",
        "TRAIN_LR_INIT               = 1e-4\n",
        "TRAIN_LR_END                = 1e-6\n",
        "TRAIN_WARMUP_EPOCHS         = 2\n",
        "TRAIN_EPOCHS                = 100\n",
        "\n",
        "# TEST options\n",
        "TEST_ANNOT_PATH             = \"mnist/mnist_test.txt\"\n",
        "TEST_BATCH_SIZE             = 4\n",
        "TEST_INPUT_SIZE             = 416\n",
        "TEST_DATA_AUG               = False\n",
        "TEST_DECTECTED_IMAGE_PATH   = \"\"\n",
        "TEST_SCORE_THRESHOLD        = 0.3\n",
        "TEST_IOU_THRESHOLD          = 0.45\n",
        "\n",
        "if TRAIN_YOLO_TINY:\n",
        "    YOLO_STRIDES            = [16, 32]    \n",
        "    # YOLO_ANCHORS            = [[[23, 27],  [37, 58],   [81,  82]], # this line can be uncommented for default coco weights\n",
        "    YOLO_ANCHORS            = [[[10, 14],  [23, 27],   [37, 58]],\n",
        "                               [[81,  82], [135, 169], [344, 319]]]"
      ],
      "metadata": {
        "id": "74MQSSLwzyrA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}