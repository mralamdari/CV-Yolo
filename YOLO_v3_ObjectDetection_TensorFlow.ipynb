{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO_v3_ObjectDetection_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xRxAg9QZNUr77hH4X7ZCBpIWho-PqwTo",
      "authorship_tag": "ABX9TyPv27UbM2it7m/LWoZa/7ST",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/CV-Yolo/blob/main/YOLO_v3_ObjectDetection_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source to this code is [here](https://www.kaggle.com/code/aruchomu/yolo-v3-object-detection-in-tensorflow)"
      ],
      "metadata": {
        "id": "yYKhNIcwEZuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PIL\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from seaborn import color_palette\n",
        "from IPython.display import display\n",
        "from PIL import ImageDraw, ImageFont"
      ],
      "metadata": {
        "id": "UO0Jt4fiByMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rETg2Ib5bDfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eccd4de1-a544-4710-b5a6-bbb5eba92f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data-for-yolo-v3-kernel.zip to /content\n",
            " 99% 264M/267M [00:02<00:00, 131MB/s] \n",
            "100% 267M/267M [00:02<00:00, 114MB/s]\n",
            "Archive:  data-for-yolo-v3-kernel.zip\n",
            "  inflating: coco.names              \n",
            "  inflating: detections.gif          \n",
            "  inflating: dog.jpg                 \n",
            "  inflating: futur.ttf               \n",
            "  inflating: office.jpg              \n",
            "  inflating: yolov3.weights          \n"
          ]
        }
      ],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d aruchomu/data-for-yolo-v3-kernel\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_BATCH_NORM_DECAY = 0.9\n",
        "_BATCH_NORM_EPSILON = 1e-5\n",
        "_LEAKY_RELU = 0.1\n",
        "_ANCHORS = [(10, 13), (16, 30), (33, 23),\n",
        "            (30, 61), (62, 45), (59, 119),\n",
        "            (116, 90), (156, 198), (373, 326)]\n",
        "\n",
        "_MODEL_SIZE = (416, 416)"
      ],
      "metadata": {
        "id": "7LdiD_zyDFbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_norm(inputs, training, data_format):\n",
        "   return tf.keras.layers.BatchNormalization(axis=1 if data_format == 'channels_first' else 3,\n",
        "                                              momentum=_BATCH_NORM_DECAY,\n",
        "                                              epsilon=_BATCH_NORM_EPSILON,\n",
        "                                              scale=True,\n",
        "                                              trainable=training)(inputs)"
      ],
      "metadata": {
        "id": "9-Tw-43rECdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fixed_padding(inputs, kernel_size, data_format):\n",
        "    pad_total = kernel_size - 1\n",
        "    pad_beg = pad_total // 2\n",
        "    pad_end = pad_total - pad_beg\n",
        "\n",
        "    if data_format == 'channels_first':\n",
        "      padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n",
        "                                      [pad_beg, pad_end],\n",
        "                                       [pad_beg, pad_end]])  \n",
        "    else:\n",
        "      padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n",
        "                                       [pad_beg, pad_end], [0, 0]])\n",
        "    return padded_inputs"
      ],
      "metadata": {
        "id": "vK5MYMBbGCLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n",
        "    if strides > 1:\n",
        "      inputs = fixed_padding(inputs, kernel_size, data_format)\n",
        "    \n",
        "    \n",
        "    return tf.keras.layers.Conv2D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  strides=strides,\n",
        "                                  padding=('SAME' if strides == 1 else 'VALID'),\n",
        "                                  use_bias=False,\n",
        "                                  data_format=data_format)(inputs)                               "
      ],
      "metadata": {
        "id": "M0ID-k0QGIGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction: Darknet-53\n"
      ],
      "metadata": {
        "id": "7dx4Y501IbeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def darknet53_residual_block(inputs, filters, training, data_format, strides=1):\n",
        "  shortcut = inputs\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, strides=strides, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=2*filters, kernel_size=3, strides=strides, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "  inputs += shortcut\n",
        "\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "dGY5WMzPIT71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def darknet53(inputs, training, data_format):\n",
        "  \n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "  \n",
        "  inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
        "\n",
        "  inputs = darknet53_residual_block(inputs, filters=32, training=training, data_format=data_format)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3, strides=2, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "\n",
        "  for o in range(2):\n",
        "      inputs = darknet53_residual_block(inputs, filters=64, training=training, data_format=data_format)\n",
        "\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3, strides=2, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "\n",
        "  for i in range(8):\n",
        "      inputs = darknet53_residual_block(inputs, filters=128, training=training, data_format=data_format)\n",
        "\n",
        "\n",
        "  route1 = inputs\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3, strides=2, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "  for i in range(8):\n",
        "      inputs = darknet53_residual_block(inputs, filters=256, training=training, data_format=data_format)\n",
        "\n",
        "  route2 = inputs\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3, strides=2, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "\n",
        "  for i in range(4):\n",
        "      inputs = darknet53_residual_block(inputs, filters=512, training=training, data_format=data_format)\n",
        " \n",
        "  return route1, route2, inputs  "
      ],
      "metadata": {
        "id": "UkMiY5G0KKil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolution layers\n"
      ],
      "metadata": {
        "id": "VIPF6nuyaLeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_convolution_block(inputs, filters, training, data_format):\n",
        "\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=2*filters, kernel_size=3, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=2*filters, kernel_size=3, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "  route = inputs\n",
        "\n",
        "  inputs = conv2d_fixed_padding(inputs, filters=2*filters, kernel_size=3, data_format=data_format)\n",
        "  inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
        "  inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "  return route, inputs"
      ],
      "metadata": {
        "id": "u0C5IG6jZMHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detection layers\n"
      ],
      "metadata": {
        "id": "H-zOe8gEfZiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n",
        "  \n",
        "  n_anchors = len(anchors)\n",
        "  inputs = tf.keras.layers.Conv2D(filters=n_anchors * (5 + n_classes),\n",
        "                                  kernel_size=1,\n",
        "                                  strides=1,\n",
        "                                  use_bias=True,\n",
        "                                  data_format=data_format)(inputs)\n",
        "\n",
        "  shape = inputs.get_shape().as_list()\n",
        "  grid_shape = shape[2: 4] if data_format == 'channels_first' else shape[1: 3]\n",
        "  \n",
        "  if data_format == 'channels_first':\n",
        "    inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
        "    \n",
        "  inputs = tf.reshape(inputs, [-1, n_anchors*grid_shape[0]*grid_shape[1], 5+n_classes])\n",
        "\n",
        "  strides = (img_size[0] // grid_shape[0], img_size[1]//grid_shape[1])\n",
        "\n",
        "  box_centers, box_shapes, confidence, classes = tf.split(inputs, [2,2,1,n_classes], axis=-1)\n",
        "  x = tf.range(grid_shape[0], dtype=tf.float32)\n",
        "  y = tf.range(grid_shape[1], dtype=tf.float32)\n",
        "  x_offset, y_offset = tf.meshgrid(x, y)\n",
        "  x_offset = tf.reshape(x_offset, (-1, 1))\n",
        "  y_offset = tf.reshape(y_offset, (-1, 1))\n",
        "  x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n",
        "  x_y_offset = tf.tile(x_y_offset, [1, n_anchors])\n",
        "  x_y_offset = tf.reshape(x_y_offset, [1, -1, 2])\n",
        "  box_centers = tf.keras.activations.sigmoid(box_centers)\n",
        "  box_centers = (box_centers + x_y_offset) * strides\n",
        "  \n",
        "  anchors = tf.tile(anchors, [grid_shape[0]*grid_shape[1], 1])\n",
        "  box_shapes = tf.exp(box_shapes) * tf.cast(anchors, tf.float32)\n",
        "  confidence = tf.keras.activations.sigmoid(confidence)\n",
        "  classes = tf.keras.activations.sigmoid(classes)\n",
        "\n",
        "  inputs = tf.concat([box_centers, box_shapes, confidence, classes], axis=-1)\n",
        "\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "4E87xE_ahch5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upsample layer"
      ],
      "metadata": {
        "id": "ztM3PP3aic1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample(inputs, out_shape, data_format):\n",
        "  if data_format == 'channels_first':\n",
        "    inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
        "    new_height = out_shape[3]\n",
        "    new_width = out_shape[2]\n",
        "  else:\n",
        "    new_height = out_shape[2]\n",
        "    new_width = out_shape[1]\n",
        "  \n",
        "  inputs = tf.image.resize(inputs, (new_height, new_width), method='nearest')\n",
        "\n",
        "  if data_format == 'channels_first':\n",
        "    inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
        "  \n",
        "  return inputs"
      ],
      "metadata": {
        "id": "7rLiWM1XiYXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-max suppression"
      ],
      "metadata": {
        "id": "Cx_EsXwqturx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_boxes(inputs):\n",
        "  center_x, center_y, width, height, confidence, classes = tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n",
        "  top_left_x = center_x - width / 2\n",
        "  top_left_y = center_y - height / 2\n",
        "  bottom_right_x = center_x + width / 2\n",
        "  bottom_right_y = center_y + height / 2\n",
        "\n",
        "  boxes = tf.concat([top_left_x, top_left_y, bottom_right_x, bottom_right_y, confidence, classes], axis=-1)\n",
        "  return boxes"
      ],
      "metadata": {
        "id": "VRmSnX8IjwLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold, confidence_threshold):\n",
        "  batch = tf.unstack(inputs)\n",
        "  boxes_dicts = []\n",
        "  for boxes in batch:\n",
        "    boxes = tf.boolean_mask(boxes, boxes[:, 4]>confidence_threshold)\n",
        "    classes = tf.argmax(boxes[:, 5:], axis=-1)\n",
        "    classes = tf.expand_dims(tf.cast(classes, tf.float32), axis=-1)\n",
        "    boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n",
        "\n",
        "    boxes_dict = dict()\n",
        "    for cls in range(n_classes):\n",
        "      mask = tf.equal(boxes[:, 5], cls)\n",
        "      mask_shape = mask.get_shape()\n",
        "      if mask_shape.ndims != 0:\n",
        "        class_boxes = tf.boolean_mask(boxes, mask)\n",
        "        boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes, [4, 1, -1], axis=-1)\n",
        "        boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n",
        "        indices = tf.image.non_max_suppression(boxes_coords, boxes_conf_scores, max_output_size, iou_threshold)\n",
        "        class_boxes = tf.gather(class_boxes, indices)\n",
        "        boxes_dict[cls] = class_boxes[:, :5]\n",
        "        boxes_dicts.append(boxes_dict)\n",
        "        return boxes_dicts"
      ],
      "metadata": {
        "id": "JHnEJSNLtyoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final model class\n"
      ],
      "metadata": {
        "id": "Go6miVqXK5Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Yolo_v3:\n",
        "  def __init__(self, n_classes, model_size, max_output_size, iou_threshold, confidence_threshold, data_format=None):\n",
        "    if not data_format:\n",
        "      if not tf.test.is_built_with_cuda():\n",
        "          data_format = 'channels_first'\n",
        "      else:\n",
        "          data_format = 'channels_last'\n",
        "\n",
        "    self.n_classes = n_classes\n",
        "    self.model_size = model_size\n",
        "    self.max_output_size = max_output_size\n",
        "    self.iou_threshold = iou_threshold\n",
        "    self.confidence_threshold = confidence_threshold\n",
        "    self.data_format = data_format\n",
        "\n",
        "  def __call__(self, inputs, training):\n",
        "\n",
        "\n",
        "    with tf.compat.v1.variable_scope('yolo_v3_model'):\n",
        "    # with tf.name_scope('yolo_v3_model'):\n",
        "\n",
        "      if self.data_format == 'channels_first':\n",
        "        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
        "      inputs = inputs / 255\n",
        "\n",
        "\n",
        "      route1, route2, inputs = darknet53(inputs, training=training, data_format=self.data_format)\n",
        "\n",
        "      route, inputs = yolo_convolution_block(inputs, filters=512, training=training, data_format=self.data_format)\n",
        "\n",
        "      detect1 = yolo_layer(inputs, n_classes=self.n_classes, img_size=self.model_size, anchors=_ANCHORS[6: 9], data_format=self.data_format)\n",
        "\n",
        "      inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1, data_format=self.data_format)\n",
        "      inputs = batch_norm(inputs, training=training, data_format=self.data_format)\n",
        "      inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "\n",
        "\n",
        "      upsample_size = route2.get_shape().as_list()\n",
        "      inputs = upsample(inputs, out_shape=upsample_size, data_format=self.data_format)\n",
        "      \n",
        "      axis = 1 if self.data_format == 'channels_first' else 3\n",
        "\n",
        "      inputs = tf.concat([inputs, route2], axis=axis)\n",
        "      route, inputs = yolo_convolution_block(inputs, filters=256, training=training, data_format=self.data_format)\n",
        "      \n",
        "      detect2 = yolo_layer(inputs, n_classes=self.n_classes, img_size=self.model_size, anchors=_ANCHORS[3: 6], data_format=self.data_format)\n",
        "      \n",
        "      inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1, data_format=self.data_format)\n",
        "      inputs = batch_norm(inputs, training=training, data_format=self.data_format)\n",
        "      inputs = tf.keras.layers.LeakyReLU(alpha=_LEAKY_RELU)(inputs)\n",
        "      \n",
        "      upsample_size = route1.get_shape().as_list()\n",
        "\n",
        "      inputs = upsample(inputs, out_shape=upsample_size, data_format=self.data_format)\n",
        "      \n",
        "      inputs = tf.concat([inputs, route1], axis=axis)\n",
        "\n",
        "      route, inputs = yolo_convolution_block(inputs, filters=128, training=training, data_format=self.data_format)\n",
        "      \n",
        "      detect3 = yolo_layer(inputs, n_classes=self.n_classes, img_size=self.model_size, anchors=_ANCHORS[0: 3], data_format=self.data_format)\n",
        "      \n",
        "      inputs = tf.concat([detect1, detect2, detect3], axis=1)\n",
        "      inputs = build_boxes(inputs)    \n",
        "      boxes_dicts = non_max_suppression(inputs,\n",
        "                                        n_classes=self.n_classes,\n",
        "                                        max_output_size=self.max_output_size,\n",
        "                                        iou_threshold=self.iou_threshold,\n",
        "                                        confidence_threshold=self.confidence_threshold)\n",
        "      return boxes_dicts"
      ],
      "metadata": {
        "id": "8N9A5DXvwwTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Utility functions\n"
      ],
      "metadata": {
        "id": "qfWNMrEAQrQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images(img_names, model_size):\n",
        "  imgs = []\n",
        "  for img_name in img_names:\n",
        "    img = PIL.Image.open(img_name)\n",
        "    img = img.resize(size=model_size)\n",
        "    img = np.array(img, dtype=np.float32)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    imgs.append(img)\n",
        "\n",
        "  imgs = np.concatenate(imgs)\n",
        "\n",
        "  return imgs"
      ],
      "metadata": {
        "id": "TRLc_SjwLHWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_class_names(file_name):\n",
        "  with open(file_name, 'r') as f:\n",
        "    class_names = f.read().splitlines()\n",
        "  return class_names"
      ],
      "metadata": {
        "id": "Fq44m3m3by38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
        "  colors = ((np.array(color_palette('hls', 80)) * 255)).astype(np.uint8)\n",
        "  for num ,img_name, boxes_dict in zip(range(len(img_names)), img_names, boxes_dicts):\n",
        "    img = PIL.Image.open(img_name)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    font = ImageFont.truetype(font='/content/futur.ttf', size=(img.size[0]+img.size[1])//100)\n",
        "    # font = PIL.ImageFont.truetype(font='../input/futur.ttf', size=(img.size[0]+img.size[1])//100)\n",
        "    resize_factor = (img.size[0]/model_size[0], img.size[1]/model_size[1])\n",
        "\n",
        "    for cls in range(len(class_names)):\n",
        "      boxes = boxes_dict[cls]    \n",
        "      if np.size(boxes) != 0:\n",
        "        color = colors[cls]\n",
        "        for box in boxes:\n",
        "          xy, confidence = box[:4], box[4]\n",
        "          xy = [xy[i] * resize_factor[i%2] for i in range(4)] \n",
        "          x0, y0 = xy[0], xy[1]\n",
        "          thickness = (img.size[0]+img.size[1]) // 200\n",
        "          for t in np.linspace(0, 1, thickness):\n",
        "            xy[0], xy[1] = xy[0] + t, xy[1] + t\n",
        "            xy[2], xy[3] = xy[2] - t, xy[3] - t\n",
        "            draw.rectangle(xy, outline=tuple(color))\n",
        "          text = f'{class_names[cls]} {confidence*100:.1f}%'\n",
        "          text_size = draw.textsize(text, font=font)\n",
        "          draw.rectangle([x0, y0-text_size[1], x0+text_size[0], y0], fill=tuple(color))\n",
        "          draw.text((x0, y0-text_size[1]), text, fill='black', fint=font)\n",
        "    display(img)"
      ],
      "metadata": {
        "id": "9WNtVZMZcHj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Converting weights to Tensorflow format"
      ],
      "metadata": {
        "id": "E4lZ4uQ1nkEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def load_weights(variables, file_name):\n",
        "  with open(file_name, 'rb') as f:\n",
        "    np.fromfile(f, dtype=np.int32, count=5)\n",
        "    weights = np.fromfile(f, dtype=np.float32)\n",
        "\n",
        "    assign_ops = []\n",
        "    ptr = 0\n",
        "    for i in range(52):\n",
        "      conv_var = variables[5 * i]\n",
        "      gamma, beta, mean, variance = variables[5 * i + 1: 5 * i + 5]\n",
        "      batch_norm_vars = [beta, gamma, mean, variance]\n",
        "\n",
        "      for var in batch_norm_vars:\n",
        "        shape = var.shape.as_list()\n",
        "        num_params = np.prod(shape)\n",
        "        var_weights = weights[ptr: ptr+num_params].reshape(shape)\n",
        "        ptr += num_params\n",
        "        assign_ops.append(tf.compat.v1.assign(var, var_weights))\n",
        "\n",
        "      shape = conv_var.shape.as_list()\n",
        "      num_params = np.prod(shape)\n",
        "      var_weights = weights[ptr: ptr+num_params].reshape((shape[3], shape[2], shape[0], shape[1]))\n",
        "      var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
        "      ptr += num_params\n",
        "      assign_ops.append(tf.compat.v1.assign(conv_var, var_weights))\n",
        "\n",
        "    ranges = [range(0, 6), range(6, 13), range(13, 20)]\n",
        "    unnormalized = [6, 13, 20]  \n",
        "    for j in range(3):\n",
        "      for i in ranges[j]:\n",
        "        current = 52*5 + 5*i + j*2\n",
        "        conv_var = variables[current]\n",
        "        gamma, beta, mean, variance = variables[current+1: current+5]\n",
        "        batch_norm_vars = [beta, gamma, mean, variance]\n",
        "\n",
        "        for var in batch_norm_vars:\n",
        "          shape = var.shape.as_list()\n",
        "          num_params = np.prod(shape)\n",
        "          var_weights = weights[ptr: ptr + num_params].reshape(shape)\n",
        "          ptr += num_params\n",
        "          assign_ops.append(tf.compat.v1.assign(var, var_weights))\n",
        "        \n",
        "        shape = conv_var.shape.as_list()\n",
        "        num_params = np.prod(shape)\n",
        "        var_weights = weights[ptr: ptr+num_params].reshape((shape[3], shape[2], shape[0], shape[1]))\n",
        "        var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
        "        ptr += num_params\n",
        "        assign_ops.append(tf.compat.v1.assign(conv_var, var_weights))\n",
        "      \n",
        "\n",
        "      bias = variables[52*5 + unnormalized[j]*5 + 2*j + 1]\n",
        "      shape = bias.shape.as_list()\n",
        "      num_params = np.prod(shape)\n",
        "      var_weights = weights[ptr: ptr+num_params].reshape(shape)\n",
        "      ptr += num_params\n",
        "      assign_ops.append(tf.compat.v1.assign(bias, var_weights))\n",
        "\n",
        "      conv_var = variables[52*5 + unnormalized[j]*5 + j*2]\n",
        "      shape = conv_var.shape.as_list()\n",
        "      num_params = np.prod(shape)\n",
        "      var_weights = weights[ptr: ptr + num_params].reshape((shape[3], shape[2], shape[0], shape[1]))\n",
        "      var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
        "      ptr += num_params\n",
        "      assign_ops.append(tf.compat.v1.assign(conv_var, var_weights))\n",
        "\n",
        "  return assign_ops"
      ],
      "metadata": {
        "id": "h2Tbd3PSnr3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample images\n"
      ],
      "metadata": {
        "id": "QNoEthjN5VyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img_names = ['../input/dog.jpg', '../input/office.jpg']\n",
        "img_names = ['/content/dog.jpg', '/content/office.jpg']\n",
        "\n",
        "# for img in img_names: display(PIL.Image.open(img))"
      ],
      "metadata": {
        "id": "PC2Ag86lnlXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detections"
      ],
      "metadata": {
        "id": "gn8Ma4Ih5jhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = len(img_names)\n",
        "batch = load_images(img_names, model_size=_MODEL_SIZE)\n",
        "# class_names = load_class_names('../input/coco.names')\n",
        "class_names = load_class_names('/content/coco.names')\n",
        "n_classes = len(class_names)\n",
        "max_output_size = 10\n",
        "iou_threshold = 0.5\n",
        "confidence_threshold = 0.5\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "model = Yolo_v3(n_classes=n_classes,\n",
        "                model_size=_MODEL_SIZE,\n",
        "                max_output_size = max_output_size,\n",
        "                iou_threshold = iou_threshold,\n",
        "                confidence_threshold = confidence_threshold)\n",
        "\n",
        "inputs = tf.keras.Input(batch_size=2, shape=[416, 416, 3], dtype=tf.float32)\n",
        "\n",
        "detections = model(inputs, training=False)\n",
        "\n",
        "model_vars = tf.compat.v1.global_variables(scope='yolo_v3_model')\n",
        "# model_vars = tf.Variable(name='yolo_v3_model')\n",
        "# assign_ops = load_weights(model_vars, '../input/yolov3.weights')\n",
        "assign_ops = load_weights(model_vars, 'yolov3.weights')\n",
        "\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(assign_ops)\n",
        "  # detection_results = sess.run(detections, feed_dict={input: batch})\n",
        "\n",
        "draw_boxes(img_names, detection_results, class_names, _MODEL_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "kkaBNNr0nlaf",
        "outputId": "3d777e78-c5ea-437e-94dd-e64536a0babb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-fd9e9c4012bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;31m# detection_results = sess.run(detections, feed_dict={input: batch})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_MODEL_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'detection_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detection_results = model(batch, training=False)\n",
        "draw_boxes(img_names, detection_results, class_names, _MODEL_SIZE)"
      ],
      "metadata": {
        "id": "TKCAZkEFV_pr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "dd52ce94-ea84-45cd-e324-1d9ac59826ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m   3205\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: \n        'Tensor' object has no attribute 'size'.\n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-ca95cf7f5ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdetection_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_MODEL_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-96cacd842fb1>\u001b[0m in \u001b[0;36mdraw_boxes\u001b[0;34m(img_names, boxes_dicts, class_names, model_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msize\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m   3206\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3209\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m         \" a NumPy call, which is not supported\".format(self.name))\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (yolo_v3_model_1/strided_slice_4:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detection_results"
      ],
      "metadata": {
        "id": "EZF7fGSgL3Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [{0: array([], shape=(0, 5), dtype=float32),\n",
        "#   1: array([[ 53.764053 ,  73.602325 , 311.8631   , 338.1856   ,   0.9879289]],\n",
        "#         dtype=float32),\n",
        "#   2: array([], shape=(0, 5), dtype=float32),\n",
        "#   3: array([], shape=(0, 5), dtype=float32),\n",
        "#   4: array([], shape=(0, 5), dtype=float32),\n",
        "#   5: array([], shape=(0, 5), dtype=float32),\n",
        "#   6: array([], shape=(0, 5), dtype=float32),\n",
        "#   7: array([[254.065    ,  61.700058 , 375.93774  , 120.224014 ,   0.9987494]],\n",
        "#         dtype=float32),\n",
        "#   8: array([], shape=(0, 5), dtype=float32),\n",
        "#   9: array([], shape=(0, 5), dtype=float32),\n",
        "#   10: array([], shape=(0, 5), dtype=float32),\n",
        "#   11: array([], shape=(0, 5), dtype=float32),\n",
        "#   12: array([], shape=(0, 5), dtype=float32),\n",
        "#   13: array([], shape=(0, 5), dtype=float32),\n",
        "#   14: array([], shape=(0, 5), dtype=float32),\n",
        "#   15: array([], shape=(0, 5), dtype=float32),\n",
        "#   16: array([[ 66.42835   , 161.5116    , 173.37656   , 391.97095   ,\n",
        "#             0.99986005]], dtype=float32),\n",
        "#   17: array([], shape=(0, 5), dtype=float32),\n",
        "#   18: array([], shape=(0, 5), dtype=float32),\n",
        "#   19: array([], shape=(0, 5), dtype=float32),\n",
        "#   20: array([], shape=(0, 5), dtype=float32),\n",
        "#   21: array([], shape=(0, 5), dtype=float32),\n",
        "#   22: array([], shape=(0, 5), dtype=float32),\n",
        "#   23: array([], shape=(0, 5), dtype=float32),\n",
        "#   24: array([], shape=(0, 5), dtype=float32),\n",
        "#   25: array([], shape=(0, 5), dtype=float32),\n",
        "#   26: array([], shape=(0, 5), dtype=float32),\n",
        "#   27: array([], shape=(0, 5), dtype=float32),\n",
        "#   28: array([], shape=(0, 5), dtype=float32),\n",
        "#   29: array([], shape=(0, 5), dtype=float32),\n",
        "#   30: array([], shape=(0, 5), dtype=float32),\n",
        "#   31: array([], shape=(0, 5), dtype=float32),\n",
        "#   32: array([], shape=(0, 5), dtype=float32),\n",
        "#   33: array([], shape=(0, 5), dtype=float32),\n",
        "#   34: array([], shape=(0, 5), dtype=float32),\n",
        "#   35: array([], shape=(0, 5), dtype=float32),\n",
        "#   36: array([], shape=(0, 5), dtype=float32),\n",
        "#   37: array([], shape=(0, 5), dtype=float32),\n",
        "#   38: array([], shape=(0, 5), dtype=float32),\n",
        "#   39: array([], shape=(0, 5), dtype=float32),\n",
        "#   40: array([], shape=(0, 5), dtype=float32),\n",
        "#   41: array([], shape=(0, 5), dtype=float32),\n",
        "#   42: array([], shape=(0, 5), dtype=float32),\n",
        "#   43: array([], shape=(0, 5), dtype=float32),\n",
        "#   44: array([], shape=(0, 5), dtype=float32),\n",
        "#   45: array([], shape=(0, 5), dtype=float32),\n",
        "#   46: array([], shape=(0, 5), dtype=float32),\n",
        "#   47: array([], shape=(0, 5), dtype=float32),\n",
        "#   48: array([], shape=(0, 5), dtype=float32),\n",
        "#   49: array([], shape=(0, 5), dtype=float32),\n",
        "#   50: array([], shape=(0, 5), dtype=float32),\n",
        "#   51: array([], shape=(0, 5), dtype=float32),\n",
        "#   52: array([], shape=(0, 5), dtype=float32),\n",
        "#   53: array([], shape=(0, 5), dtype=float32),\n",
        "#   54: array([], shape=(0, 5), dtype=float32),\n",
        "#   55: array([], shape=(0, 5), dtype=float32),\n",
        "#   56: array([], shape=(0, 5), dtype=float32),\n",
        "#   57: array([], shape=(0, 5), dtype=float32),\n",
        "#   58: array([], shape=(0, 5), dtype=float32),\n",
        "#   59: array([], shape=(0, 5), dtype=float32),\n",
        "#   60: array([], shape=(0, 5), dtype=float32),\n",
        "#   61: array([], shape=(0, 5), dtype=float32),\n",
        "#   62: array([], shape=(0, 5), dtype=float32),\n",
        "#   63: array([], shape=(0, 5), dtype=float32),\n",
        "#   64: array([], shape=(0, 5), dtype=float32),\n",
        "#   65: array([], shape=(0, 5), dtype=float32),\n",
        "#   66: array([], shape=(0, 5), dtype=float32),\n",
        "#   67: array([], shape=(0, 5), dtype=float32),\n",
        "#   68: array([], shape=(0, 5), dtype=float32),\n",
        "#   69: array([], shape=(0, 5), dtype=float32),\n",
        "#   70: array([], shape=(0, 5), dtype=float32),\n",
        "#   71: array([], shape=(0, 5), dtype=float32),\n",
        "#   72: array([], shape=(0, 5), dtype=float32),\n",
        "#   73: array([], shape=(0, 5), dtype=float32),\n",
        "#   74: array([], shape=(0, 5), dtype=float32),\n",
        "#   75: array([], shape=(0, 5), dtype=float32),\n",
        "#   76: array([], shape=(0, 5), dtype=float32),\n",
        "#   77: array([], shape=(0, 5), dtype=float32),\n",
        "#   78: array([], shape=(0, 5), dtype=float32),\n",
        "#   79: array([], shape=(0, 5), dtype=float32)},\n",
        "#  {0: array([[303.460, 140.577, 358.287, 300.84616,0.9982315 ],\n",
        "#   [ 44.398, 143.606,  80.84187, 249.56879,0.9774136 ],\n",
        "#   [264.01596, 164.30782, 289.17648, 289.189,0.974046  ],\n",
        "#   [283.107, 162.23547, 304.975, 277.641,0.84681517],\n",
        "#   [206.82169, 194.46164, 232.27643, 240.44531, 0.6003503 ]], dtype=float32),\n",
        "#   1: array([], shape=(0, 5), dtype=float32),\n",
        "#   2: array([], shape=(0, 5), dtype=float32),\n",
        "#   3: array([], shape=(0, 5), dtype=float32),\n",
        "#   4: array([], shape=(0, 5), dtype=float32),\n",
        "#   5: array([], shape=(0, 5), dtype=float32),\n",
        "#   6: array([], shape=(0, 5), dtype=float32),\n",
        "#   7: array([], shape=(0, 5), dtype=float32),\n",
        "#   8: array([], shape=(0, 5), dtype=float32),\n",
        "#   9: array([], shape=(0, 5), dtype=float32),\n",
        "#   10: array([], shape=(0, 5), dtype=float32),\n",
        "#   11: array([], shape=(0, 5), dtype=float32),\n",
        "#   12: array([], shape=(0, 5), dtype=float32),\n",
        "#   13: array([], shape=(0, 5), dtype=float32),\n",
        "#   14: array([], shape=(0, 5), dtype=float32),\n",
        "#   15: array([], shape=(0, 5), dtype=float32),\n",
        "#   16: array([], shape=(0, 5), dtype=float32),\n",
        "#   17: array([], shape=(0, 5), dtype=float32),\n",
        "#   18: array([], shape=(0, 5), dtype=float32),\n",
        "#   19: array([], shape=(0, 5), dtype=float32),\n",
        "#   20: array([], shape=(0, 5), dtype=float32),\n",
        "#   21: array([], shape=(0, 5), dtype=float32),\n",
        "#   22: array([], shape=(0, 5), dtype=float32),\n",
        "#   23: array([], shape=(0, 5), dtype=float32),\n",
        "#   24: array([], shape=(0, 5), dtype=float32),\n",
        "#   25: array([], shape=(0, 5), dtype=float32),\n",
        "#   26: array([], shape=(0, 5), dtype=float32),\n",
        "#   27: array([], shape=(0, 5), dtype=float32),\n",
        "#   28: array([], shape=(0, 5), dtype=float32),\n",
        "#   29: array([], shape=(0, 5), dtype=float32),\n",
        "#   30: array([], shape=(0, 5), dtype=float32),\n",
        "#   31: array([], shape=(0, 5), dtype=float32),\n",
        "#   32: array([], shape=(0, 5), dtype=float32),\n",
        "#   33: array([], shape=(0, 5), dtype=float32),\n",
        "#   34: array([], shape=(0, 5), dtype=float32),\n",
        "#   35: array([], shape=(0, 5), dtype=float32),\n",
        "#   36: array([], shape=(0, 5), dtype=float32),\n",
        "#   37: array([], shape=(0, 5), dtype=float32),\n",
        "#   38: array([], shape=(0, 5), dtype=float32),\n",
        "#   39: array([], shape=(0, 5), dtype=float32),\n",
        "#   40: array([], shape=(0, 5), dtype=float32),\n",
        "#   41: array([], shape=(0, 5), dtype=float32),\n",
        "#   42: array([], shape=(0, 5), dtype=float32),\n",
        "#   43: array([], shape=(0, 5), dtype=float32),\n",
        "#   44: array([], shape=(0, 5), dtype=float32),\n",
        "#   45: array([], shape=(0, 5), dtype=float32),\n",
        "#   46: array([], shape=(0, 5), dtype=float32),\n",
        "#   47: array([], shape=(0, 5), dtype=float32),\n",
        "#   48: array([], shape=(0, 5), dtype=float32),\n",
        "#   49: array([], shape=(0, 5), dtype=float32),\n",
        "#   50: array([], shape=(0, 5), dtype=float32),\n",
        "#   51: array([], shape=(0, 5), dtype=float32),\n",
        "#   52: array([], shape=(0, 5), dtype=float32),\n",
        "#   53: array([], shape=(0, 5), dtype=float32),\n",
        "#   54: array([], shape=(0, 5), dtype=float32),\n",
        "#   55: array([], shape=(0, 5), dtype=float32),\n",
        "#   56: array([[ 93.927666  , 291.33884   , 148.7069    , 407.8115    ,\n",
        "#             0.99375993],\n",
        "#          [218.47237   , 238.70616   , 244.73384   , 327.30655   ,\n",
        "#             0.847579  ],\n",
        "#          [246.76869   , 233.07529   , 270.86276   , 285.96335   ,\n",
        "#             0.746225  ],\n",
        "#          [170.81442   , 244.8394    , 221.33908   , 343.35895   ,\n",
        "#             0.57724875]], dtype=float32),\n",
        "#   57: array([], shape=(0, 5), dtype=float32),\n",
        "#   58: array([], shape=(0, 5), dtype=float32),\n",
        "#   59: array([], shape=(0, 5), dtype=float32),\n",
        "#   60: array([], shape=(0, 5), dtype=float32),\n",
        "#   61: array([], shape=(0, 5), dtype=float32),\n",
        "#   62: array([[175.47862  , 201.59422  , 206.7936   , 241.96846  ,   0.9417429]],\n",
        "#         dtype=float32),\n",
        "#   63: array([], shape=(0, 5), dtype=float32),\n",
        "#   64: array([], shape=(0, 5), dtype=float32),\n",
        "#   65: array([], shape=(0, 5), dtype=float32),\n",
        "#   66: array([], shape=(0, 5), dtype=float32),\n",
        "#   67: array([], shape=(0, 5), dtype=float32),\n",
        "#   68: array([], shape=(0, 5), dtype=float32),\n",
        "#   69: array([], shape=(0, 5), dtype=float32),\n",
        "#   70: array([], shape=(0, 5), dtype=float32),\n",
        "#   71: array([], shape=(0, 5), dtype=float32),\n",
        "#   72: array([], shape=(0, 5), dtype=float32),\n",
        "#   73: array([], shape=(0, 5), dtype=float32),\n",
        "#   74: array([], shape=(0, 5), dtype=float32),\n",
        "#   75: array([], shape=(0, 5), dtype=float32),\n",
        "#   76: array([], shape=(0, 5), dtype=float32),\n",
        "#   77: array([], shape=(0, 5), dtype=float32),\n",
        "#   78: array([], shape=(0, 5), dtype=float32),\n",
        "#   79: array([], shape=(0, 5), dtype=float32)}]"
      ],
      "metadata": {
        "id": "yFSkOG0AM0Pq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}